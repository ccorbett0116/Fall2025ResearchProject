{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/ccorbett0116/Fall2025ResearchProject/blob/main/Research_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZVMwQDnKO8XY"
   },
   "source": [
    "# Project Title:\n",
    "# Authors: Jose Henriquez, Cole Corbett\n",
    "## Description:\n",
    "The deployment of medical AI systems across different hospitals raises critical questions about whether fairness and representation quality can be reliably transferred across clinical domains. Models trained on one hospital’s imaging data are often reused in new environments where patient demographics, imaging devices, and diagnostic practices differ substantially, potentially resulting in unintended bias against certain groups. This project investigates this challenge by studying fairness-aware representation alignment in medical imaging. The student will train contrastive learning models—such as SimCLR—independently on two large-scale chest X-ray datasets: CheXpert (from Stanford Hospital) and MIMIC-CXR (from Beth Israel Deaconess Medical Center). After learning embeddings in each domain, the student will apply domain alignment techniques such as Procrustes alignment to map representations from the CheXpert embedding space into the MIMIC-CXR space. The aligned embeddings will then be evaluated using fairness metrics designed for representation spaces, including demographic subgroup alignment, intra- vs. inter-group embedding disparity, and cluster-level demographic parity. The expected outcome is a rigorous understanding of whether fairness properties learned in one hospital setting preserve, degrade, or improve when transferred to another, revealing how robust model fairness is to realworld clinical domain shifts. A practical use case involves a healthcare network seeking to deploy a model trained at a major academic hospital (e.g., Stanford) into a community hospital setting: this project helps determine whether the transferred representations remain equitable across patient groups such as older adults, women, or specific disease cohorts. The findings will support responsible AI deployment in healthcare by highlighting the conditions under which fairness is stable across institutions and identifying scenarios where domain-specific mitigation strategies may be required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-17T22:40:57.226897Z",
     "start_time": "2025-11-17T22:40:46.108146Z"
    },
    "id": "uhUKfIU5G_5u"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: kagglehub in c:\\users\\joseh\\.conda\\envs\\py312\\lib\\site-packages (0.3.13)\n",
      "Requirement already satisfied: polars in c:\\users\\joseh\\.conda\\envs\\py312\\lib\\site-packages (1.35.2)\n",
      "Requirement already satisfied: packaging in c:\\users\\joseh\\.conda\\envs\\py312\\lib\\site-packages (from kagglehub) (24.2)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\joseh\\.conda\\envs\\py312\\lib\\site-packages (from kagglehub) (6.0.3)\n",
      "Requirement already satisfied: requests in c:\\users\\joseh\\.conda\\envs\\py312\\lib\\site-packages (from kagglehub) (2.32.5)\n",
      "Requirement already satisfied: tqdm in c:\\users\\joseh\\.conda\\envs\\py312\\lib\\site-packages (from kagglehub) (4.67.1)\n",
      "Requirement already satisfied: polars-runtime-32==1.35.2 in c:\\users\\joseh\\.conda\\envs\\py312\\lib\\site-packages (from polars) (1.35.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\joseh\\.conda\\envs\\py312\\lib\\site-packages (from requests->kagglehub) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\joseh\\.conda\\envs\\py312\\lib\\site-packages (from requests->kagglehub) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\joseh\\.conda\\envs\\py312\\lib\\site-packages (from requests->kagglehub) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\joseh\\.conda\\envs\\py312\\lib\\site-packages (from requests->kagglehub) (2025.11.12)\n",
      "Requirement already satisfied: colorama in c:\\users\\joseh\\.conda\\envs\\py312\\lib\\site-packages (from tqdm->kagglehub) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "#Process is probably different on colab, this is hyperspecific to me because I'm working on Pycharm connected to my WSL\n",
    "import sys\n",
    "import os\n",
    "!{sys.executable} -m pip install kagglehub polars\n",
    "#We're going to use polars because it's significantly faster, it's build on rust and enables multi-threaded processing as well as some memory optimizations over pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-17T23:02:52.542252Z",
     "start_time": "2025-11-17T23:02:52.164261Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\joseh\\.conda\\envs\\py312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to chexpert dataset files: C:\\Users\\joseh\\.cache\\kagglehub\\datasets\\mimsadiislam\\chexpert\\versions\\1\n",
      "Path to mimic dataset files: C:\\Users\\joseh\\.cache\\kagglehub\\datasets\\simhadrisadaram\\mimic-cxr-dataset\\versions\\2\n"
     ]
    }
   ],
   "source": [
    "#Again, this is probably different on colab\n",
    "import kagglehub\n",
    "path_chexpert = kagglehub.dataset_download(\"mimsadiislam/chexpert\")\n",
    "print(\"Path to chexpert dataset files:\", path_chexpert)\n",
    "path_mimic = kagglehub.dataset_download(\"simhadrisadaram/mimic-cxr-dataset\")\n",
    "print(\"Path to mimic dataset files:\", path_mimic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-17T23:03:33.697687Z",
     "start_time": "2025-11-17T23:03:33.694832Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "os.makedirs(\"./CheXpert/train\", exist_ok=True)\n",
    "os.makedirs(\"./CheXpert/val\", exist_ok=True)\n",
    "os.makedirs(\"./MIMIC-CXR/train\", exist_ok=True)\n",
    "os.makedirs(\"./MIMIC-CXR/val\", exist_ok=True)\n",
    "os.makedirs(\"./checkpoints\", exist_ok=True)\n",
    "os.makedirs(\"./embeddings\", exist_ok=True)\n",
    "os.makedirs(\"./logs\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-17T23:06:02.171042Z",
     "start_time": "2025-11-17T23:06:02.117046Z"
    }
   },
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "\n",
    "\n",
    "dir_chexpert = os.path.join(path_chexpert, \"CheXpert-v1.0-small\")\n",
    "dir_mimic = path_mimic\n",
    "\n",
    "train_csv_chexpert = os.path.join(dir_chexpert, \"train.csv\")\n",
    "train_csv_mimic = os.path.join(dir_mimic, \"mimic_cxr_aug_train.csv\")\n",
    "valid_csv_chexpert = os.path.join(dir_chexpert, \"valid.csv\")\n",
    "valid_csv_mimic = os.path.join(dir_mimic, \"mimic_cxr_aug_validate.csv\")\n",
    "\n",
    "df_train_chexpert = pl.read_csv(train_csv_chexpert)\n",
    "df_train_mimic = pl.read_csv(train_csv_mimic)\n",
    "df_valid_chexpert = pl.read_csv(valid_csv_chexpert)\n",
    "df_valid_mimic = pl.read_csv(valid_csv_mimic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying CheXpert Train images to ./CheXpert/train...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 223414/223414 [03:51<00:00, 963.76it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed: 223414 files copied, 0 files skipped\n",
      "\n",
      "Copying CheXpert Val images to ./CheXpert/val...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 234/234 [00:00<00:00, 767.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed: 234 files copied, 0 files skipped\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(234, 0)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def copy_images_polars(df, source_dir, target_dir, path_column=\"Path\", dataset_name=\"Dataset\"):\n",
    "    \"\"\"\n",
    "    Copies images from source_dir to target_dir based on the paths in Polars DataFrame.\n",
    "    Preserves patient/study folder structure to avoid filename collisions.\n",
    "    \"\"\"\n",
    "    copied = 0\n",
    "    skipped = 0\n",
    "    print(f\"Copying {dataset_name} images to {target_dir}...\")\n",
    "    \n",
    "    for row in tqdm(df.iter_rows(), total=len(df)):\n",
    "        if path_column in df.columns:\n",
    "            idx = df.columns.index(path_column)\n",
    "            rel_path = row[idx]\n",
    "        else:\n",
    "            rel_path = row[0]\n",
    "        \n",
    "        # Fix: remove duplicated base folder for CheXpert\n",
    "        if dataset_name.startswith(\"CheXpert\"):\n",
    "            rel_path_fixed = rel_path.replace(\"CheXpert-v1.0-small/\", \"\")\n",
    "        else:\n",
    "            rel_path_fixed = rel_path\n",
    "        \n",
    "        src = os.path.join(source_dir, rel_path_fixed)\n",
    "        \n",
    "        # PRESERVE STRUCTURE: Keep last 2 folder levels (patient/study)\n",
    "        path_parts = Path(rel_path_fixed).parts\n",
    "        if len(path_parts) >= 3:\n",
    "            # Keep patient/study folders\n",
    "            subfolder = os.path.join(path_parts[-3], path_parts[-2])\n",
    "        else:\n",
    "            subfolder = \"\"\n",
    "        \n",
    "        # Create subfolder structure in target\n",
    "        target_subfolder = os.path.join(target_dir, subfolder)\n",
    "        os.makedirs(target_subfolder, exist_ok=True)\n",
    "        \n",
    "        dst = os.path.join(target_subfolder, os.path.basename(rel_path_fixed))\n",
    "        \n",
    "        if os.path.exists(src):\n",
    "            shutil.copy2(src, dst)\n",
    "            copied += 1\n",
    "        else:\n",
    "            skipped += 1\n",
    "            if skipped <= 5:\n",
    "                print(f\"Warning: Source file not found: {src}\")\n",
    "    \n",
    "    print(f\"Completed: {copied} files copied, {skipped} files skipped\\n\")\n",
    "    return copied, skipped\n",
    "\n",
    "copy_images_polars(df_train_chexpert, dir_chexpert, \"./CheXpert/train\", path_column=\"Path\", dataset_name=\"CheXpert Train\")\n",
    "copy_images_polars(df_valid_chexpert, dir_chexpert, \"./CheXpert/val\", path_column=\"Path\", dataset_name=\"CheXpert Val\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MIMIC image root: C:\\Users\\joseh\\.cache\\kagglehub\\datasets\\simhadrisadaram\\mimic-cxr-dataset\\versions\\2\\official_data_iccv_final\n",
      "Copying MIMIC Train images to ./MIMIC-CXR/train...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 12/64586 [00:00<10:24, 103.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Source file not found: files/p10/p10000935/s50578979/d0b71acc-b5a62046-bbb5f6b8-7b173b85-65cdf738.jpg\n",
      "Warning: Source file not found: files/p10/p10000935/s51178377/3be619d1-506a66cf-ff1ab8a1-2efb77bb-fe7d59fc.jpg\n",
      "Warning: Source file not found: files/p10/p10000935/s51178377/9b314ad7-fbcb0422-6db62dfc-732858d0-a5527d8b.jpg\n",
      "Warning: Source file not found: files/p10/p10000935/s55697293/6a5c3985-7764bdd0-ec5a6a74-af78bcaa-4ca33ec3.jpg\n",
      "Warning: Source file not found: files/p10/p10000935/s55697293/c50494f1-90e2bff5-e9189550-1a4562fd-6ab5204c.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 64586/64586 [06:00<00:00, 179.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed: 259038 files copied, 109922 files skipped, 0 parse errors\n",
      "\n",
      "Using MIMIC image root: C:\\Users\\joseh\\.cache\\kagglehub\\datasets\\simhadrisadaram\\mimic-cxr-dataset\\versions\\2\\official_data_iccv_final\n",
      "Copying MIMIC Validation images to ./MIMIC-CXR/val...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▎         | 18/500 [00:00<00:03, 148.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Source file not found: files/p10/p10057482/s52168780/5799175e-c125dfdb-3bd28e88-4f9ad41b-37bdd2ed.jpg\n",
      "Warning: Source file not found: files/p10/p10057482/s52168780/5eb9cc4f-c43b5757-52310877-3c87b128-465b02db.jpg\n",
      "Warning: Source file not found: files/p10/p10190940/s50438069/2aafe5ea-12d26b26-972e16c4-ff3d0f9a-ae75d498.jpg\n",
      "Warning: Source file not found: files/p10/p10190940/s50438069/707c7ae4-04900b82-789fd588-1d86b741-ec38124b.jpg\n",
      "Warning: Source file not found: files/p10/p10190940/s51351116/13490b6f-3eb75751-a191991b-e8f33cad-e423992c.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:02<00:00, 169.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed: 2099 files copied, 892 files skipped, 0 parse errors\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2099, 892)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import ast\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ============================================================================\n",
    "# MIMIC Image Copy Function - Flat Structure (matches CheXpert style)\n",
    "# ============================================================================\n",
    "\n",
    "def find_mimic_image_root(dataset_root):\n",
    "    \"\"\"Find where MIMIC images are actually stored\"\"\"\n",
    "    possible_roots = [\n",
    "        \"official_data_iccv_final\",\n",
    "        \"files\",\n",
    "        \"images\",\n",
    "        \"mimic-cxr\",\n",
    "    ]\n",
    "    \n",
    "    for root in possible_roots:\n",
    "        test_path = os.path.join(dataset_root, root)\n",
    "        if os.path.exists(test_path):\n",
    "            return test_path\n",
    "    \n",
    "    return dataset_root\n",
    "\n",
    "\n",
    "def copy_mimic_images_polars(df, source_dir, target_dir, path_column=\"image\", dataset_name=\"MIMIC\"):\n",
    "    \"\"\"\n",
    "    Copies MIMIC images from source_dir to target_dir (flat structure).\n",
    "    \n",
    "    Args:\n",
    "        df: Polars DataFrame with image paths\n",
    "        source_dir: Root directory of MIMIC dataset\n",
    "        target_dir: Where to copy images\n",
    "        path_column: Column name containing image paths (default: \"image\")\n",
    "        dataset_name: Name for logging\n",
    "    \"\"\"\n",
    "    os.makedirs(target_dir, exist_ok=True)\n",
    "    \n",
    "    copied = 0\n",
    "    skipped = 0\n",
    "    parse_errors = 0\n",
    "    \n",
    "    # Find where images are actually stored\n",
    "    images_root = find_mimic_image_root(source_dir)\n",
    "    print(f\"Using MIMIC image root: {images_root}\")\n",
    "    print(f\"Copying {dataset_name} images to {target_dir}...\")\n",
    "    \n",
    "    for row in tqdm(df.iter_rows(), total=len(df)):\n",
    "        # Get the image column value\n",
    "        if path_column in df.columns:\n",
    "            idx = df.columns.index(path_column)\n",
    "            image_value = row[idx]\n",
    "        else:\n",
    "            image_value = row[0]\n",
    "        \n",
    "        # Parse the list of image paths\n",
    "        try:\n",
    "            # MIMIC stores image paths as string representation of list\n",
    "            if isinstance(image_value, str):\n",
    "                image_list = ast.literal_eval(image_value)\n",
    "            elif isinstance(image_value, list):\n",
    "                image_list = image_value\n",
    "            else:\n",
    "                parse_errors += 1\n",
    "                continue\n",
    "        except Exception as e:\n",
    "            parse_errors += 1\n",
    "            if parse_errors <= 5:\n",
    "                print(f\"Warning: Could not parse image paths: {image_value[:100]}...\")\n",
    "            continue\n",
    "        \n",
    "        # Process each image in the list\n",
    "        for img_path in image_list:\n",
    "            # Clean the path\n",
    "            img_path = img_path.lstrip(\"/\\\\\").replace(\"\\\\\", \"/\")\n",
    "            \n",
    "            # Try multiple path combinations\n",
    "            path_attempts = [\n",
    "                img_path,\n",
    "                img_path.replace(\"files/\", \"\"),  # Remove 'files/' prefix\n",
    "                os.path.join(\"files\", img_path.replace(\"files/\", \"\")),  # Ensure 'files/' prefix\n",
    "            ]\n",
    "            \n",
    "            found = False\n",
    "            for attempt in path_attempts:\n",
    "                src = os.path.join(images_root, attempt)\n",
    "                \n",
    "                if os.path.exists(src):\n",
    "                    # Copy to target directory\n",
    "                    dst = os.path.join(target_dir, os.path.basename(src))\n",
    "                    \n",
    "                    # Handle potential duplicate filenames\n",
    "                    if os.path.exists(dst):\n",
    "                        # Add counter to filename\n",
    "                        base_name, ext = os.path.splitext(os.path.basename(src))\n",
    "                        counter = 1\n",
    "                        while os.path.exists(os.path.join(target_dir, f\"{base_name}_{counter}{ext}\")):\n",
    "                            counter += 1\n",
    "                        dst = os.path.join(target_dir, f\"{base_name}_{counter}{ext}\")\n",
    "                    \n",
    "                    shutil.copy2(src, dst)\n",
    "                    copied += 1\n",
    "                    found = True\n",
    "                    break\n",
    "            \n",
    "            if not found:\n",
    "                skipped += 1\n",
    "                if skipped <= 5:\n",
    "                    print(f\"Warning: Source file not found: {img_path}\")\n",
    "    \n",
    "    print(f\"Completed: {copied} files copied, {skipped} files skipped, {parse_errors} parse errors\\n\")\n",
    "    return copied, skipped\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 4️⃣ Copy MIMIC images\n",
    "# =========================\n",
    "copy_mimic_images_polars(df_train_mimic, path_mimic, \"./MIMIC-CXR/train\", \n",
    "                         path_column=\"image\", dataset_name=\"MIMIC Train\")\n",
    "copy_mimic_images_polars(df_valid_mimic, path_mimic, \"./MIMIC-CXR/val\", \n",
    "                         path_column=\"image\", dataset_name=\"MIMIC Validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ORGANIZING MIMIC-CXR FOR SOLO-LEARN\n",
      "================================================================================\n",
      "Organizing ./MIMIC-CXR/train -> ./MIMIC-CXR-organized/train/xray/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 259038/259038 [00:17<00:00, 15031.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Organized 259038 images into ./MIMIC-CXR-organized/train\\xray\n",
      "Organizing ./MIMIC-CXR/val -> ./MIMIC-CXR-organized/val/xray/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2099/2099 [00:00<00:00, 15265.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Organized 2099 images into ./MIMIC-CXR-organized/val\\xray\n",
      "\n",
      "================================================================================\n",
      "ORGANIZING CheXpert FOR SOLO-LEARN\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "def organize_for_sololearn(flat_dir, output_dir, class_name=\"images\"):\n",
    "    \"\"\"\n",
    "    Reorganize flat image directory into solo-learn expected format\n",
    "    \n",
    "    Before: flat_dir/img1.jpg, img2.jpg, ...\n",
    "    After:  output_dir/class_name/img1.jpg, img2.jpg, ...\n",
    "    \"\"\"\n",
    "    print(f\"Organizing {flat_dir} -> {output_dir}/{class_name}/\")\n",
    "    \n",
    "    # Create class subfolder\n",
    "    class_dir = os.path.join(output_dir, class_name)\n",
    "    os.makedirs(class_dir, exist_ok=True)\n",
    "    \n",
    "    # Find all images in flat directory\n",
    "    image_extensions = {'.jpg', '.jpeg', '.png'}\n",
    "    image_files = []\n",
    "    \n",
    "    for ext in image_extensions:\n",
    "        image_files.extend(Path(flat_dir).glob(f\"*{ext}\"))\n",
    "    \n",
    "    # Move/copy images to class folder\n",
    "    for img_file in tqdm(image_files):\n",
    "        dst = os.path.join(class_dir, img_file.name)\n",
    "        if not os.path.exists(dst):\n",
    "            shutil.copy2(img_file, dst)\n",
    "    \n",
    "    print(f\"✓ Organized {len(image_files)} images into {class_dir}\")\n",
    "    return len(image_files)\n",
    "\n",
    "\n",
    "# Organize MIMIC\n",
    "print(\"=\"*80)\n",
    "print(\"ORGANIZING MIMIC-CXR FOR SOLO-LEARN\")\n",
    "print(\"=\"*80)\n",
    "organize_for_sololearn(\"./MIMIC-CXR/train\", \"./MIMIC-CXR-organized/train\", \"xray\")\n",
    "organize_for_sololearn(\"./MIMIC-CXR/val\", \"./MIMIC-CXR-organized/val\", \"xray\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Processing TRAIN set ===\n",
      "Scanning for images...\n",
      "Found 223414 images total.\n",
      "Starting to link 223414 images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Linking: 100%|██████████| 223414/223414 [01:27<00:00, 2559.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Linked 0/223414 images successfully.\n",
      "Failed links: 223414\n",
      "First 5 errors: [('view1_frontal.jpg', \"[WinError 183] Cannot create a file when that file already exists: 'CheXpert\\\\\\\\train\\\\\\\\patient00001\\\\\\\\study1\\\\\\\\view1_frontal.jpg' -> 'chexpert-organized\\\\\\\\train\\\\\\\\xray\\\\\\\\train_patient00001_study1_view1_frontal.jpg'\"), ('view1_frontal.jpg', \"[WinError 183] Cannot create a file when that file already exists: 'CheXpert\\\\\\\\train\\\\\\\\patient00002\\\\\\\\study1\\\\\\\\view1_frontal.jpg' -> 'chexpert-organized\\\\\\\\train\\\\\\\\xray\\\\\\\\train_patient00002_study1_view1_frontal.jpg'\"), ('view2_lateral.jpg', \"[WinError 183] Cannot create a file when that file already exists: 'CheXpert\\\\\\\\train\\\\\\\\patient00002\\\\\\\\study1\\\\\\\\view2_lateral.jpg' -> 'chexpert-organized\\\\\\\\train\\\\\\\\xray\\\\\\\\train_patient00002_study1_view2_lateral.jpg'\"), ('view1_frontal.jpg', \"[WinError 183] Cannot create a file when that file already exists: 'CheXpert\\\\\\\\train\\\\\\\\patient00002\\\\\\\\study2\\\\\\\\view1_frontal.jpg' -> 'chexpert-organized\\\\\\\\train\\\\\\\\xray\\\\\\\\train_patient00002_study2_view1_frontal.jpg'\"), ('view1_frontal.jpg', \"[WinError 183] Cannot create a file when that file already exists: 'CheXpert\\\\\\\\train\\\\\\\\patient00003\\\\\\\\study1\\\\\\\\view1_frontal.jpg' -> 'chexpert-organized\\\\\\\\train\\\\\\\\xray\\\\\\\\train_patient00003_study1_view1_frontal.jpg'\")]\n",
      "Done! Train images are now in: chexpert-organized/train/xray/\n",
      "\n",
      "=== Processing VALIDATION set ===\n",
      "Scanning for images...\n",
      "Found 234 images total.\n",
      "Starting to link 234 images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Linking: 100%|██████████| 234/234 [00:00<00:00, 2818.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Linked 0/234 images successfully.\n",
      "Failed links: 234\n",
      "First 5 errors: [('view1_frontal.jpg', \"[WinError 183] Cannot create a file when that file already exists: 'CheXpert\\\\\\\\val\\\\\\\\patient64541\\\\\\\\study1\\\\\\\\view1_frontal.jpg' -> 'chexpert-organized\\\\\\\\val\\\\\\\\xray\\\\\\\\val_patient64541_study1_view1_frontal.jpg'\"), ('view1_frontal.jpg', \"[WinError 183] Cannot create a file when that file already exists: 'CheXpert\\\\\\\\val\\\\\\\\patient64542\\\\\\\\study1\\\\\\\\view1_frontal.jpg' -> 'chexpert-organized\\\\\\\\val\\\\\\\\xray\\\\\\\\val_patient64542_study1_view1_frontal.jpg'\"), ('view2_lateral.jpg', \"[WinError 183] Cannot create a file when that file already exists: 'CheXpert\\\\\\\\val\\\\\\\\patient64542\\\\\\\\study1\\\\\\\\view2_lateral.jpg' -> 'chexpert-organized\\\\\\\\val\\\\\\\\xray\\\\\\\\val_patient64542_study1_view2_lateral.jpg'\"), ('view1_frontal.jpg', \"[WinError 183] Cannot create a file when that file already exists: 'CheXpert\\\\\\\\val\\\\\\\\patient64543\\\\\\\\study1\\\\\\\\view1_frontal.jpg' -> 'chexpert-organized\\\\\\\\val\\\\\\\\xray\\\\\\\\val_patient64543_study1_view1_frontal.jpg'\"), ('view1_frontal.jpg', \"[WinError 183] Cannot create a file when that file already exists: 'CheXpert\\\\\\\\val\\\\\\\\patient64544\\\\\\\\study1\\\\\\\\view1_frontal.jpg' -> 'chexpert-organized\\\\\\\\val\\\\\\\\xray\\\\\\\\val_patient64544_study1_view1_frontal.jpg'\")]\n",
      "Done! Val images are now in: chexpert-organized/val/xray/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "def count_images(flat_dir):\n",
    "    \"\"\"Count images in directory without copying\"\"\"\n",
    "    flat_path = Path(flat_dir)\n",
    "    print(\"Scanning for images...\")\n",
    "    image_files = [\n",
    "        f for f in flat_path.rglob(\"*\")\n",
    "        if f.is_file() and f.suffix.lower() in (\".jpg\", \".jpeg\", \".png\")\n",
    "    ]\n",
    "    print(f\"Found {len(image_files)} images total.\")\n",
    "    return image_files\n",
    "\n",
    "def copy_images_simple(image_files, output_dir, class_name=\"images\"):\n",
    "    \"\"\"Link images one at a time with progress bar (much faster than copying)\"\"\"\n",
    "    class_dir = Path(output_dir) / class_name\n",
    "    class_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    print(f\"Starting to link {len(image_files)} images...\")\n",
    "    successful = 0\n",
    "    failed = []\n",
    "    \n",
    "    for img_file in tqdm(image_files, desc=\"Linking\"):\n",
    "        try:\n",
    "            # Create unique filename from full path\n",
    "            # e.g., patient00001/study1/view1_frontal.jpg -> patient00001_study1_view1_frontal.jpg\n",
    "            relative_path = img_file.relative_to(img_file.parents[3])  # Get path from train/val root\n",
    "            unique_name = str(relative_path).replace(os.sep, '_')\n",
    "            \n",
    "            dst = class_dir / unique_name\n",
    "            \n",
    "            # Use hard link instead of copy (instantaneous, no disk space used)\n",
    "            os.link(img_file, dst)\n",
    "            successful += 1\n",
    "            \n",
    "        except Exception as e:\n",
    "            failed.append((img_file.name, str(e)))\n",
    "    \n",
    "    print(f\"\\nLinked {successful}/{len(image_files)} images successfully.\")\n",
    "    if failed:\n",
    "        print(f\"Failed links: {len(failed)}\")\n",
    "        print(\"First 5 errors:\", failed[:5])\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # TRAIN SET\n",
    "    print(\"\\n=== Processing TRAIN set ===\")\n",
    "    train_images = count_images(\"./CheXpert/train\")\n",
    "    copy_images_simple(train_images, \"chexpert-organized/train\", class_name=\"xray\")\n",
    "    print(\"Done! Train images are now in: chexpert-organized/train/xray/\")\n",
    "    \n",
    "    # VALIDATION SET\n",
    "    print(\"\\n=== Processing VALIDATION set ===\")\n",
    "    val_images = count_images(\"./CheXpert/val\")\n",
    "    copy_images_simple(val_images, \"chexpert-organized/val\", class_name=\"xray\")\n",
    "    print(\"Done! Val images are now in: chexpert-organized/val/xray/\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-17T23:06:04.059255Z",
     "start_time": "2025-11-17T23:06:04.055880Z"
    }
   },
   "outputs": [],
   "source": [
    "df_train_chexpert.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-17T23:06:08.544149Z",
     "start_time": "2025-11-17T23:06:08.540252Z"
    }
   },
   "outputs": [],
   "source": [
    "df_train_mimic.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training with solo learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "solo-learn added to path.\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.path.abspath(\"solo-learn\"))\n",
    "print(\"solo-learn added to path.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hydra reset.\n"
     ]
    }
   ],
   "source": [
    "#reset hydra\n",
    "from hydra.core.global_hydra import GlobalHydra\n",
    "GlobalHydra.instance().clear()\n",
    "print(\"Hydra reset.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset: custom\n",
      "data:\n",
      "  train_path: CheXpert-organized/train\n",
      "  val_path: CheXpert-organized/val\n",
      "  format: image_folder\n",
      "  batch_size: 128\n",
      "  num_workers: 8\n",
      "backbone:\n",
      "  name: resnet18\n",
      "  kwargs: {}\n",
      "method_kwargs:\n",
      "  proj_hidden_dim: 2048\n",
      "  proj_output_dim: 128\n",
      "  temperature: 0.5\n",
      "optimizer:\n",
      "  name: adam\n",
      "  lr: 0.001\n",
      "  weight_decay: 1.0e-06\n",
      "scheduler:\n",
      "  name: cosine\n",
      "  warmup_epochs: 10\n",
      "max_epochs: 2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load config\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "cfg = OmegaConf.load(\"config/simclr.yaml\")\n",
    "print(OmegaConf.to_yaml(cfg))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data loaders\n",
    "from solo.data.classification_dataloader import prepare_data\n",
    "train_loader, val_loader = prepare_data(\n",
    "    dataset=cfg.dataset,\n",
    "    train_data_path=cfg.data.train_path,\n",
    "    val_data_path=cfg.data.val_path,\n",
    "    data_format=\"image_folder\",\n",
    "    batch_size=cfg.data.batch_size,\n",
    "    num_workers=cfg.data.num_workers,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting seaborn\n",
      "  Using cached seaborn-0.13.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: numpy!=1.24.0,>=1.20 in c:\\users\\joseh\\.conda\\envs\\py312\\lib\\site-packages (from seaborn) (2.3.3)\n",
      "Requirement already satisfied: pandas>=1.2 in c:\\users\\joseh\\.conda\\envs\\py312\\lib\\site-packages (from seaborn) (2.3.3)\n",
      "Collecting matplotlib!=3.6.1,>=3.4 (from seaborn)\n",
      "  Downloading matplotlib-3.10.7-cp312-cp312-win_amd64.whl.metadata (11 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib!=3.6.1,>=3.4->seaborn)\n",
      "  Downloading contourpy-1.3.3-cp312-cp312-win_amd64.whl.metadata (5.5 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib!=3.6.1,>=3.4->seaborn)\n",
      "  Using cached cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib!=3.6.1,>=3.4->seaborn)\n",
      "  Downloading fonttools-4.60.1-cp312-cp312-win_amd64.whl.metadata (114 kB)\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib!=3.6.1,>=3.4->seaborn)\n",
      "  Downloading kiwisolver-1.4.9-cp312-cp312-win_amd64.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\joseh\\.conda\\envs\\py312\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\joseh\\.conda\\envs\\py312\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (11.3.0)\n",
      "Collecting pyparsing>=3 (from matplotlib!=3.6.1,>=3.4->seaborn)\n",
      "  Using cached pyparsing-3.2.5-py3-none-any.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\joseh\\.conda\\envs\\py312\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\joseh\\.conda\\envs\\py312\\lib\\site-packages (from pandas>=1.2->seaborn) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\joseh\\.conda\\envs\\py312\\lib\\site-packages (from pandas>=1.2->seaborn) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\joseh\\.conda\\envs\\py312\\lib\\site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn) (1.17.0)\n",
      "Using cached seaborn-0.13.2-py3-none-any.whl (294 kB)\n",
      "Downloading matplotlib-3.10.7-cp312-cp312-win_amd64.whl (8.1 MB)\n",
      "   ---------------------------------------- 0.0/8.1 MB ? eta -:--:--\n",
      "   ---------------------------- ----------- 5.8/8.1 MB 27.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 8.1/8.1 MB 29.7 MB/s  0:00:00\n",
      "Downloading contourpy-1.3.3-cp312-cp312-win_amd64.whl (226 kB)\n",
      "Using cached cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Downloading fonttools-4.60.1-cp312-cp312-win_amd64.whl (2.3 MB)\n",
      "   ---------------------------------------- 0.0/2.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.3/2.3 MB 64.7 MB/s  0:00:00\n",
      "Downloading kiwisolver-1.4.9-cp312-cp312-win_amd64.whl (73 kB)\n",
      "Using cached pyparsing-3.2.5-py3-none-any.whl (113 kB)\n",
      "Installing collected packages: pyparsing, kiwisolver, fonttools, cycler, contourpy, matplotlib, seaborn\n",
      "\n",
      "   ---------------------------------------- 0/7 [pyparsing]\n",
      "   ----------- ---------------------------- 2/7 [fonttools]\n",
      "   ----------- ---------------------------- 2/7 [fonttools]\n",
      "   ----------- ---------------------------- 2/7 [fonttools]\n",
      "   ----------- ---------------------------- 2/7 [fonttools]\n",
      "   ----------- ---------------------------- 2/7 [fonttools]\n",
      "   ----------- ---------------------------- 2/7 [fonttools]\n",
      "   ----------- ---------------------------- 2/7 [fonttools]\n",
      "   ----------- ---------------------------- 2/7 [fonttools]\n",
      "   ----------- ---------------------------- 2/7 [fonttools]\n",
      "   ----------- ---------------------------- 2/7 [fonttools]\n",
      "   ----------- ---------------------------- 2/7 [fonttools]\n",
      "   ----------- ---------------------------- 2/7 [fonttools]\n",
      "   ----------- ---------------------------- 2/7 [fonttools]\n",
      "   ----------- ---------------------------- 2/7 [fonttools]\n",
      "   ----------- ---------------------------- 2/7 [fonttools]\n",
      "   ----------- ---------------------------- 2/7 [fonttools]\n",
      "   ----------- ---------------------------- 2/7 [fonttools]\n",
      "   ----------- ---------------------------- 2/7 [fonttools]\n",
      "   ----------- ---------------------------- 2/7 [fonttools]\n",
      "   ----------- ---------------------------- 2/7 [fonttools]\n",
      "   ----------- ---------------------------- 2/7 [fonttools]\n",
      "   ----------- ---------------------------- 2/7 [fonttools]\n",
      "   ----------- ---------------------------- 2/7 [fonttools]\n",
      "   ----------- ---------------------------- 2/7 [fonttools]\n",
      "   ----------- ---------------------------- 2/7 [fonttools]\n",
      "   ----------- ---------------------------- 2/7 [fonttools]\n",
      "   ----------- ---------------------------- 2/7 [fonttools]\n",
      "   ----------- ---------------------------- 2/7 [fonttools]\n",
      "   ----------- ---------------------------- 2/7 [fonttools]\n",
      "   ----------- ---------------------------- 2/7 [fonttools]\n",
      "   ----------- ---------------------------- 2/7 [fonttools]\n",
      "   ----------- ---------------------------- 2/7 [fonttools]\n",
      "   ---------------------- ----------------- 4/7 [contourpy]\n",
      "   ---------------------------- ----------- 5/7 [matplotlib]\n",
      "   ---------------------------- ----------- 5/7 [matplotlib]\n",
      "   ---------------------------- ----------- 5/7 [matplotlib]\n",
      "   ---------------------------- ----------- 5/7 [matplotlib]\n",
      "   ---------------------------- ----------- 5/7 [matplotlib]\n",
      "   ---------------------------- ----------- 5/7 [matplotlib]\n",
      "   ---------------------------- ----------- 5/7 [matplotlib]\n",
      "   ---------------------------- ----------- 5/7 [matplotlib]\n",
      "   ---------------------------- ----------- 5/7 [matplotlib]\n",
      "   ---------------------------- ----------- 5/7 [matplotlib]\n",
      "   ---------------------------- ----------- 5/7 [matplotlib]\n",
      "   ---------------------------- ----------- 5/7 [matplotlib]\n",
      "   ---------------------------- ----------- 5/7 [matplotlib]\n",
      "   ---------------------------- ----------- 5/7 [matplotlib]\n",
      "   ---------------------------- ----------- 5/7 [matplotlib]\n",
      "   ---------------------------- ----------- 5/7 [matplotlib]\n",
      "   ---------------------------- ----------- 5/7 [matplotlib]\n",
      "   ---------------------------- ----------- 5/7 [matplotlib]\n",
      "   ---------------------------- ----------- 5/7 [matplotlib]\n",
      "   ---------------------------- ----------- 5/7 [matplotlib]\n",
      "   ---------------------------- ----------- 5/7 [matplotlib]\n",
      "   ---------------------------- ----------- 5/7 [matplotlib]\n",
      "   ---------------------------- ----------- 5/7 [matplotlib]\n",
      "   ---------------------------- ----------- 5/7 [matplotlib]\n",
      "   ---------------------------- ----------- 5/7 [matplotlib]\n",
      "   ---------------------------- ----------- 5/7 [matplotlib]\n",
      "   ---------------------------- ----------- 5/7 [matplotlib]\n",
      "   ---------------------------- ----------- 5/7 [matplotlib]\n",
      "   ---------------------------- ----------- 5/7 [matplotlib]\n",
      "   ---------------------------- ----------- 5/7 [matplotlib]\n",
      "   ---------------------------- ----------- 5/7 [matplotlib]\n",
      "   ---------------------------- ----------- 5/7 [matplotlib]\n",
      "   ---------------------------------- ----- 6/7 [seaborn]\n",
      "   ---------------------------------- ----- 6/7 [seaborn]\n",
      "   ---------------------------------- ----- 6/7 [seaborn]\n",
      "   ---------------------------------- ----- 6/7 [seaborn]\n",
      "   ---------------------------------------- 7/7 [seaborn]\n",
      "\n",
      "Successfully installed contourpy-1.3.3 cycler-0.12.1 fonttools-4.60.1 kiwisolver-1.4.9 matplotlib-3.10.7 pyparsing-3.2.5 seaborn-0.13.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The scripts fonttools.exe, pyftmerge.exe, pyftsubset.exe and ttx.exe are installed in 'c:\\Users\\joseh\\.conda\\envs\\py312\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n"
     ]
    }
   ],
   "source": [
    "!{sys.executable} -m pip install seaborn\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Module: solo-learn.main_knn ---\n",
      "DataLoader\n",
      "OmegaConf\n",
      "Path\n",
      "WeightedKNNClassifier\n",
      "extract_features\n",
      "main\n",
      "parse_args_knn\n",
      "prepare_dataloaders\n",
      "prepare_datasets\n",
      "prepare_transforms\n",
      "run_knn\n",
      "tqdm\n",
      "\n",
      "--- Module: solo-learn.main_linear ---\n",
      "AutoResumer\n",
      "BaseMethod\n",
      "Checkpointer\n",
      "DDPStrategy\n",
      "DictConfig\n",
      "LabelSmoothingCrossEntropy\n",
      "LearningRateMonitor\n",
      "LinearModel\n",
      "Mixup\n",
      "OmegaConf\n",
      "SoftTargetCrossEntropy\n",
      "Trainer\n",
      "WandbLogger\n",
      "main\n",
      "make_contiguous\n",
      "parse_cfg\n",
      "prepare_data\n",
      "\n",
      "--- Module: solo-learn.main_pretrain ---\n",
      "AutoResumer\n",
      "Checkpointer\n",
      "DDPStrategy\n",
      "DictConfig\n",
      "FullTransformPipeline\n",
      "LearningRateMonitor\n",
      "NCropAugmentation\n",
      "OmegaConf\n",
      "Trainer\n",
      "WandbLogger\n",
      "build_transform_pipeline\n",
      "main\n",
      "make_contiguous\n",
      "omegaconf_select\n",
      "parse_cfg\n",
      "prepare_data_classification\n",
      "prepare_dataloader\n",
      "prepare_datasets\n",
      "seed_everything\n",
      "Could not import solo-learn.main_umap: No module named 'seaborn'\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "usage: ipykernel_launcher.py [global_opts] cmd1 [cmd1_opts] [cmd2 [cmd2_opts] ...]\n   or: ipykernel_launcher.py --help [cmd1 cmd2 ...]\n   or: ipykernel_launcher.py --help-commands\n   or: ipykernel_launcher.py cmd --help\n\nerror: option --fullname must not have an argument",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[31mSystemExit\u001b[39m\u001b[31m:\u001b[39m usage: ipykernel_launcher.py [global_opts] cmd1 [cmd1_opts] [cmd2 [cmd2_opts] ...]\n   or: ipykernel_launcher.py --help [cmd1 cmd2 ...]\n   or: ipykernel_launcher.py --help-commands\n   or: ipykernel_launcher.py cmd --help\n\nerror: option --fullname must not have an argument\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import inspect\n",
    "import importlib\n",
    "\n",
    "def explore_solo_learn(path=\"solo-learn\"):\n",
    "    \"\"\"\n",
    "    Recursively prints modules, classes, and functions in solo-learn repo.\n",
    "    Helps understand what is available for model instantiation.\n",
    "    \"\"\"\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        for f in files:\n",
    "            if f.endswith(\".py\") and f != \"__init__.py\":\n",
    "                module_path = os.path.join(root, f)\n",
    "                # Convert path to importable module name\n",
    "                module_name = module_path.replace(os.sep, \".\").replace(\".py\", \"\")\n",
    "                try:\n",
    "                    module = importlib.import_module(module_name)\n",
    "                    print(f\"\\n--- Module: {module_name} ---\")\n",
    "                    for name, obj in inspect.getmembers(module):\n",
    "                        if inspect.isclass(obj) or inspect.isfunction(obj):\n",
    "                            print(name)\n",
    "                except Exception as e:\n",
    "                    print(f\"Could not import {module_name}: {e}\")\n",
    "\n",
    "# Run it\n",
    "explore_solo_learn()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3234304305.py, line 2)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mpython -m solo.main_pretrain \\\u001b[39m\n              ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# CheXpert\n",
    "!python -m solo.main_pretrain \\\n",
    "  --method simclr \\\n",
    "  --dataset custom \\\n",
    "  --data_dir /path/to/chesXpert \\\n",
    "  --train_dir train \\\n",
    "  --val_dir val \\\n",
    "  --encoder resnet50 \\\n",
    "  --batch_size 128 \\\n",
    "  --max_epochs 50 \\\n",
    "  --num_workers 8 \\\n",
    "  --gpus 1 \\\n",
    "  --precision 16 \\\n",
    "  --logger tensorboard \\\n",
    "  --log_dir ./logs \\\n",
    "  --project mimic_ssl \\\n",
    "  --name simclr_chexpert\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embedding extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CheXpert\n",
    "python -m solo.main_linear \\\n",
    "  --backbone resnet50 \\\n",
    "  --pretrained_feature_extractor ./checkpoints/simclr_chexpert.ckpt \\\n",
    "  --extract_features \\\n",
    "  --data_dir /path/to/CheXpert \\\n",
    "  --train_dir train \\\n",
    "  --output_features simclr_chexpert_embeddings.pt\n",
    "\n",
    "# MIMIC\n",
    "python -m solo.main_linear \\\n",
    "  --backbone resnet50 \\\n",
    "  --pretrained_feature_extractor ./checkpoints/simclr_mimic.ckpt \\\n",
    "  --extract_features \\\n",
    "  --data_dir /path/to/MIMIC-CXR \\\n",
    "  --train_dir train \\\n",
    "  --output_features simclr_mimic_embeddings.pt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Load embeddings\n",
    "chexpert_emb = torch.load(\"./embeddings/simclr_chexpert_embeddings.pt\")\n",
    "mimic_emb = torch.load(\"./embeddings/simclr_mimic_embeddings.pt\")\n",
    "\n",
    "# Procrustes alignment\n",
    "def procrustes_alignment(X, Y):\n",
    "    U, _, Vt = torch.linalg.svd(Y.T @ X)\n",
    "    R = U @ Vt\n",
    "    return X @ R.T, R\n",
    "\n",
    "chexpert_aligned, R = procrustes_alignment(chexpert_emb, mimic_emb)\n",
    "torch.save(chexpert_aligned, \"./embeddings/chexpert_aligned.pt\")\n",
    "\n",
    "# Example: subgroup distance with Polars\n",
    "def subgroup_centroid_distance(embeddings, demographics_col):\n",
    "    groups = demographics_col.unique().to_list()\n",
    "    centroids = {}\n",
    "    for g in groups:\n",
    "        mask = demographics_col == g\n",
    "        emb_group = embeddings[mask.to_numpy()]\n",
    "        centroids[g] = emb_group.mean(dim=0)\n",
    "    dist = {}\n",
    "    for g1 in groups:\n",
    "        for g2 in groups:\n",
    "            dist[(g1, g2)] = torch.norm(centroids[g1] - centroids[g2]).item()\n",
    "    return dist\n",
    "\n",
    "demographics = df_train_chexpert[\"Sex\"]  # or \"Gender\" column\n",
    "dists = subgroup_centroid_distance(chexpert_aligned, demographics)\n",
    "print(dists)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir ./logs\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPk3VYQAvkG8XZbveKPAqcC",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.12",
   "language": "python",
   "name": "py312"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  },
  "polyglot_notebook": {
   "kernelInfo": {
    "defaultKernelName": "csharp",
    "items": [
     {
      "aliases": [],
      "name": "csharp"
     }
    ]
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
