{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/ccorbett0116/Fall2025ResearchProject/blob/main/Research_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZVMwQDnKO8XY"
   },
   "source": [
    "# Project Title:\n",
    "# Authors: Jose Henriquez, Cole Corbett\n",
    "## Description:\n",
    "The deployment of medical AI systems across different hospitals raises critical questions about whether fairness and representation quality can be reliably transferred across clinical domains. Models trained on one hospital’s imaging data are often reused in new environments where patient demographics, imaging devices, and diagnostic practices differ substantially, potentially resulting in unintended bias against certain groups. This project investigates this challenge by studying fairness-aware representation alignment in medical imaging. The student will train contrastive learning models—such as SimCLR—independently on two large-scale chest X-ray datasets: CheXpert (from Stanford Hospital) and MIMIC-CXR (from Beth Israel Deaconess Medical Center). After learning embeddings in each domain, the student will apply domain alignment techniques such as Procrustes alignment to map representations from the CheXpert embedding space into the MIMIC-CXR space. The aligned embeddings will then be evaluated using fairness metrics designed for representation spaces, including demographic subgroup alignment, intra- vs. inter-group embedding disparity, and cluster-level demographic parity. The expected outcome is a rigorous understanding of whether fairness properties learned in one hospital setting preserve, degrade, or improve when transferred to another, revealing how robust model fairness is to realworld clinical domain shifts. A practical use case involves a healthcare network seeking to deploy a model trained at a major academic hospital (e.g., Stanford) into a community hospital setting: this project helps determine whether the transferred representations remain equitable across patient groups such as older adults, women, or specific disease cohorts. The findings will support responsible AI deployment in healthcare by highlighting the conditions under which fairness is stable across institutions and identifying scenarios where domain-specific mitigation strategies may be required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-17T22:40:57.226897Z",
     "start_time": "2025-11-17T22:40:46.108146Z"
    },
    "id": "uhUKfIU5G_5u"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: kagglehub in c:\\users\\joseh\\.conda\\envs\\py312\\lib\\site-packages (0.3.13)\n",
      "Requirement already satisfied: polars in c:\\users\\joseh\\.conda\\envs\\py312\\lib\\site-packages (1.35.2)\n",
      "Requirement already satisfied: packaging in c:\\users\\joseh\\.conda\\envs\\py312\\lib\\site-packages (from kagglehub) (24.2)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\joseh\\.conda\\envs\\py312\\lib\\site-packages (from kagglehub) (6.0.3)\n",
      "Requirement already satisfied: requests in c:\\users\\joseh\\.conda\\envs\\py312\\lib\\site-packages (from kagglehub) (2.32.5)\n",
      "Requirement already satisfied: tqdm in c:\\users\\joseh\\.conda\\envs\\py312\\lib\\site-packages (from kagglehub) (4.67.1)\n",
      "Requirement already satisfied: polars-runtime-32==1.35.2 in c:\\users\\joseh\\.conda\\envs\\py312\\lib\\site-packages (from polars) (1.35.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\joseh\\.conda\\envs\\py312\\lib\\site-packages (from requests->kagglehub) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\joseh\\.conda\\envs\\py312\\lib\\site-packages (from requests->kagglehub) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\joseh\\.conda\\envs\\py312\\lib\\site-packages (from requests->kagglehub) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\joseh\\.conda\\envs\\py312\\lib\\site-packages (from requests->kagglehub) (2025.11.12)\n",
      "Requirement already satisfied: colorama in c:\\users\\joseh\\.conda\\envs\\py312\\lib\\site-packages (from tqdm->kagglehub) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "#Process is probably different on colab, this is hyperspecific to me because I'm working on Pycharm connected to my WSL\n",
    "import sys\n",
    "import os\n",
    "!{sys.executable} -m pip install kagglehub polars\n",
    "#We're going to use polars because it's significantly faster, it's build on rust and enables multi-threaded processing as well as some memory optimizations over pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-17T23:02:52.542252Z",
     "start_time": "2025-11-17T23:02:52.164261Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\joseh\\.conda\\envs\\py312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to chexpert dataset files: C:\\Users\\joseh\\.cache\\kagglehub\\datasets\\mimsadiislam\\chexpert\\versions\\1\n",
      "Path to mimic dataset files: C:\\Users\\joseh\\.cache\\kagglehub\\datasets\\simhadrisadaram\\mimic-cxr-dataset\\versions\\2\n"
     ]
    }
   ],
   "source": [
    "#Again, this is probably different on colab\n",
    "import kagglehub\n",
    "path_chexpert = kagglehub.dataset_download(\"mimsadiislam/chexpert\")\n",
    "print(\"Path to chexpert dataset files:\", path_chexpert)\n",
    "path_mimic = kagglehub.dataset_download(\"simhadrisadaram/mimic-cxr-dataset\")\n",
    "print(\"Path to mimic dataset files:\", path_mimic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-17T23:03:33.697687Z",
     "start_time": "2025-11-17T23:03:33.694832Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "os.makedirs(\"./CheXpert/train\", exist_ok=True)\n",
    "os.makedirs(\"./CheXpert/val\", exist_ok=True)\n",
    "os.makedirs(\"./MIMIC-CXR/train\", exist_ok=True)\n",
    "os.makedirs(\"./MIMIC-CXR/val\", exist_ok=True)\n",
    "os.makedirs(\"./checkpoints\", exist_ok=True)\n",
    "os.makedirs(\"./embeddings\", exist_ok=True)\n",
    "os.makedirs(\"./logs\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-17T23:06:02.171042Z",
     "start_time": "2025-11-17T23:06:02.117046Z"
    }
   },
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "\n",
    "\n",
    "dir_chexpert = os.path.join(path_chexpert, \"CheXpert-v1.0-small\")\n",
    "dir_mimic = path_mimic\n",
    "\n",
    "train_csv_chexpert = os.path.join(dir_chexpert, \"train.csv\")\n",
    "train_csv_mimic = os.path.join(dir_mimic, \"mimic_cxr_aug_train.csv\")\n",
    "valid_csv_chexpert = os.path.join(dir_chexpert, \"valid.csv\")\n",
    "valid_csv_mimic = os.path.join(dir_mimic, \"mimic_cxr_aug_validate.csv\")\n",
    "\n",
    "df_train_chexpert = pl.read_csv(train_csv_chexpert)\n",
    "df_train_mimic = pl.read_csv(train_csv_mimic)\n",
    "df_valid_chexpert = pl.read_csv(valid_csv_chexpert)\n",
    "df_valid_mimic = pl.read_csv(valid_csv_mimic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying CheXpert Train images to ./CheXpert/train...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 223414/223414 [03:51<00:00, 963.76it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed: 223414 files copied, 0 files skipped\n",
      "\n",
      "Copying CheXpert Val images to ./CheXpert/val...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 234/234 [00:00<00:00, 767.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed: 234 files copied, 0 files skipped\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(234, 0)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def copy_images_polars(df, source_dir, target_dir, path_column=\"Path\", dataset_name=\"Dataset\"):\n",
    "    \"\"\"\n",
    "    Copies images from source_dir to target_dir based on the paths in Polars DataFrame.\n",
    "    Preserves patient/study folder structure to avoid filename collisions.\n",
    "    \"\"\"\n",
    "    copied = 0\n",
    "    skipped = 0\n",
    "    print(f\"Copying {dataset_name} images to {target_dir}...\")\n",
    "    \n",
    "    for row in tqdm(df.iter_rows(), total=len(df)):\n",
    "        if path_column in df.columns:\n",
    "            idx = df.columns.index(path_column)\n",
    "            rel_path = row[idx]\n",
    "        else:\n",
    "            rel_path = row[0]\n",
    "        \n",
    "        # Fix: remove duplicated base folder for CheXpert\n",
    "        if dataset_name.startswith(\"CheXpert\"):\n",
    "            rel_path_fixed = rel_path.replace(\"CheXpert-v1.0-small/\", \"\")\n",
    "        else:\n",
    "            rel_path_fixed = rel_path\n",
    "        \n",
    "        src = os.path.join(source_dir, rel_path_fixed)\n",
    "        \n",
    "        # PRESERVE STRUCTURE: Keep last 2 folder levels (patient/study)\n",
    "        path_parts = Path(rel_path_fixed).parts\n",
    "        if len(path_parts) >= 3:\n",
    "            # Keep patient/study folders\n",
    "            subfolder = os.path.join(path_parts[-3], path_parts[-2])\n",
    "        else:\n",
    "            subfolder = \"\"\n",
    "        \n",
    "        # Create subfolder structure in target\n",
    "        target_subfolder = os.path.join(target_dir, subfolder)\n",
    "        os.makedirs(target_subfolder, exist_ok=True)\n",
    "        \n",
    "        dst = os.path.join(target_subfolder, os.path.basename(rel_path_fixed))\n",
    "        \n",
    "        if os.path.exists(src):\n",
    "            shutil.copy2(src, dst)\n",
    "            copied += 1\n",
    "        else:\n",
    "            skipped += 1\n",
    "            if skipped <= 5:\n",
    "                print(f\"Warning: Source file not found: {src}\")\n",
    "    \n",
    "    print(f\"Completed: {copied} files copied, {skipped} files skipped\\n\")\n",
    "    return copied, skipped\n",
    "\n",
    "copy_images_polars(df_train_chexpert, dir_chexpert, \"./CheXpert/train\", path_column=\"Path\", dataset_name=\"CheXpert Train\")\n",
    "copy_images_polars(df_valid_chexpert, dir_chexpert, \"./CheXpert/val\", path_column=\"Path\", dataset_name=\"CheXpert Val\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MIMIC image root: C:\\Users\\joseh\\.cache\\kagglehub\\datasets\\simhadrisadaram\\mimic-cxr-dataset\\versions\\2\\official_data_iccv_final\n",
      "Copying MIMIC Train images to ./MIMIC-CXR/train...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 12/64586 [00:00<10:24, 103.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Source file not found: files/p10/p10000935/s50578979/d0b71acc-b5a62046-bbb5f6b8-7b173b85-65cdf738.jpg\n",
      "Warning: Source file not found: files/p10/p10000935/s51178377/3be619d1-506a66cf-ff1ab8a1-2efb77bb-fe7d59fc.jpg\n",
      "Warning: Source file not found: files/p10/p10000935/s51178377/9b314ad7-fbcb0422-6db62dfc-732858d0-a5527d8b.jpg\n",
      "Warning: Source file not found: files/p10/p10000935/s55697293/6a5c3985-7764bdd0-ec5a6a74-af78bcaa-4ca33ec3.jpg\n",
      "Warning: Source file not found: files/p10/p10000935/s55697293/c50494f1-90e2bff5-e9189550-1a4562fd-6ab5204c.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 64586/64586 [06:00<00:00, 179.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed: 259038 files copied, 109922 files skipped, 0 parse errors\n",
      "\n",
      "Using MIMIC image root: C:\\Users\\joseh\\.cache\\kagglehub\\datasets\\simhadrisadaram\\mimic-cxr-dataset\\versions\\2\\official_data_iccv_final\n",
      "Copying MIMIC Validation images to ./MIMIC-CXR/val...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▎         | 18/500 [00:00<00:03, 148.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Source file not found: files/p10/p10057482/s52168780/5799175e-c125dfdb-3bd28e88-4f9ad41b-37bdd2ed.jpg\n",
      "Warning: Source file not found: files/p10/p10057482/s52168780/5eb9cc4f-c43b5757-52310877-3c87b128-465b02db.jpg\n",
      "Warning: Source file not found: files/p10/p10190940/s50438069/2aafe5ea-12d26b26-972e16c4-ff3d0f9a-ae75d498.jpg\n",
      "Warning: Source file not found: files/p10/p10190940/s50438069/707c7ae4-04900b82-789fd588-1d86b741-ec38124b.jpg\n",
      "Warning: Source file not found: files/p10/p10190940/s51351116/13490b6f-3eb75751-a191991b-e8f33cad-e423992c.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:02<00:00, 169.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed: 2099 files copied, 892 files skipped, 0 parse errors\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2099, 892)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import ast\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ============================================================================\n",
    "# MIMIC Image Copy Function - Flat Structure (matches CheXpert style)\n",
    "# ============================================================================\n",
    "\n",
    "def find_mimic_image_root(dataset_root):\n",
    "    \"\"\"Find where MIMIC images are actually stored\"\"\"\n",
    "    possible_roots = [\n",
    "        \"official_data_iccv_final\",\n",
    "        \"files\",\n",
    "        \"images\",\n",
    "        \"mimic-cxr\",\n",
    "    ]\n",
    "    \n",
    "    for root in possible_roots:\n",
    "        test_path = os.path.join(dataset_root, root)\n",
    "        if os.path.exists(test_path):\n",
    "            return test_path\n",
    "    \n",
    "    return dataset_root\n",
    "\n",
    "\n",
    "def copy_mimic_images_polars(df, source_dir, target_dir, path_column=\"image\", dataset_name=\"MIMIC\"):\n",
    "    \"\"\"\n",
    "    Copies MIMIC images from source_dir to target_dir (flat structure).\n",
    "    \n",
    "    Args:\n",
    "        df: Polars DataFrame with image paths\n",
    "        source_dir: Root directory of MIMIC dataset\n",
    "        target_dir: Where to copy images\n",
    "        path_column: Column name containing image paths (default: \"image\")\n",
    "        dataset_name: Name for logging\n",
    "    \"\"\"\n",
    "    os.makedirs(target_dir, exist_ok=True)\n",
    "    \n",
    "    copied = 0\n",
    "    skipped = 0\n",
    "    parse_errors = 0\n",
    "    \n",
    "    # Find where images are actually stored\n",
    "    images_root = find_mimic_image_root(source_dir)\n",
    "    print(f\"Using MIMIC image root: {images_root}\")\n",
    "    print(f\"Copying {dataset_name} images to {target_dir}...\")\n",
    "    \n",
    "    for row in tqdm(df.iter_rows(), total=len(df)):\n",
    "        # Get the image column value\n",
    "        if path_column in df.columns:\n",
    "            idx = df.columns.index(path_column)\n",
    "            image_value = row[idx]\n",
    "        else:\n",
    "            image_value = row[0]\n",
    "        \n",
    "        # Parse the list of image paths\n",
    "        try:\n",
    "            # MIMIC stores image paths as string representation of list\n",
    "            if isinstance(image_value, str):\n",
    "                image_list = ast.literal_eval(image_value)\n",
    "            elif isinstance(image_value, list):\n",
    "                image_list = image_value\n",
    "            else:\n",
    "                parse_errors += 1\n",
    "                continue\n",
    "        except Exception as e:\n",
    "            parse_errors += 1\n",
    "            if parse_errors <= 5:\n",
    "                print(f\"Warning: Could not parse image paths: {image_value[:100]}...\")\n",
    "            continue\n",
    "        \n",
    "        # Process each image in the list\n",
    "        for img_path in image_list:\n",
    "            # Clean the path\n",
    "            img_path = img_path.lstrip(\"/\\\\\").replace(\"\\\\\", \"/\")\n",
    "            \n",
    "            # Try multiple path combinations\n",
    "            path_attempts = [\n",
    "                img_path,\n",
    "                img_path.replace(\"files/\", \"\"),  # Remove 'files/' prefix\n",
    "                os.path.join(\"files\", img_path.replace(\"files/\", \"\")),  # Ensure 'files/' prefix\n",
    "            ]\n",
    "            \n",
    "            found = False\n",
    "            for attempt in path_attempts:\n",
    "                src = os.path.join(images_root, attempt)\n",
    "                \n",
    "                if os.path.exists(src):\n",
    "                    # Copy to target directory\n",
    "                    dst = os.path.join(target_dir, os.path.basename(src))\n",
    "                    \n",
    "                    # Handle potential duplicate filenames\n",
    "                    if os.path.exists(dst):\n",
    "                        # Add counter to filename\n",
    "                        base_name, ext = os.path.splitext(os.path.basename(src))\n",
    "                        counter = 1\n",
    "                        while os.path.exists(os.path.join(target_dir, f\"{base_name}_{counter}{ext}\")):\n",
    "                            counter += 1\n",
    "                        dst = os.path.join(target_dir, f\"{base_name}_{counter}{ext}\")\n",
    "                    \n",
    "                    shutil.copy2(src, dst)\n",
    "                    copied += 1\n",
    "                    found = True\n",
    "                    break\n",
    "            \n",
    "            if not found:\n",
    "                skipped += 1\n",
    "                if skipped <= 5:\n",
    "                    print(f\"Warning: Source file not found: {img_path}\")\n",
    "    \n",
    "    print(f\"Completed: {copied} files copied, {skipped} files skipped, {parse_errors} parse errors\\n\")\n",
    "    return copied, skipped\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 4️⃣ Copy MIMIC images\n",
    "# =========================\n",
    "copy_mimic_images_polars(df_train_mimic, path_mimic, \"./MIMIC-CXR/train\", \n",
    "                         path_column=\"image\", dataset_name=\"MIMIC Train\")\n",
    "copy_mimic_images_polars(df_valid_mimic, path_mimic, \"./MIMIC-CXR/val\", \n",
    "                         path_column=\"image\", dataset_name=\"MIMIC Validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ORGANIZING MIMIC-CXR FOR SOLO-LEARN\n",
      "================================================================================\n",
      "Organizing ./MIMIC-CXR/train -> ./MIMIC-CXR-organized/train/xray/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 259038/259038 [00:17<00:00, 15031.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Organized 259038 images into ./MIMIC-CXR-organized/train\\xray\n",
      "Organizing ./MIMIC-CXR/val -> ./MIMIC-CXR-organized/val/xray/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2099/2099 [00:00<00:00, 15265.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Organized 2099 images into ./MIMIC-CXR-organized/val\\xray\n",
      "\n",
      "================================================================================\n",
      "ORGANIZING CheXpert FOR SOLO-LEARN\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "def organize_for_sololearn(flat_dir, output_dir, class_name=\"images\"):\n",
    "    \"\"\"\n",
    "    Reorganize flat image directory into solo-learn expected format\n",
    "    \n",
    "    Before: flat_dir/img1.jpg, img2.jpg, ...\n",
    "    After:  output_dir/class_name/img1.jpg, img2.jpg, ...\n",
    "    \"\"\"\n",
    "    print(f\"Organizing {flat_dir} -> {output_dir}/{class_name}/\")\n",
    "    \n",
    "    # Create class subfolder\n",
    "    class_dir = os.path.join(output_dir, class_name)\n",
    "    os.makedirs(class_dir, exist_ok=True)\n",
    "    \n",
    "    # Find all images in flat directory\n",
    "    image_extensions = {'.jpg', '.jpeg', '.png'}\n",
    "    image_files = []\n",
    "    \n",
    "    for ext in image_extensions:\n",
    "        image_files.extend(Path(flat_dir).glob(f\"*{ext}\"))\n",
    "    \n",
    "    # Move/copy images to class folder\n",
    "    for img_file in tqdm(image_files):\n",
    "        dst = os.path.join(class_dir, img_file.name)\n",
    "        if not os.path.exists(dst):\n",
    "            shutil.copy2(img_file, dst)\n",
    "    \n",
    "    print(f\"✓ Organized {len(image_files)} images into {class_dir}\")\n",
    "    return len(image_files)\n",
    "\n",
    "\n",
    "# Organize MIMIC\n",
    "print(\"=\"*80)\n",
    "print(\"ORGANIZING MIMIC-CXR FOR SOLO-LEARN\")\n",
    "print(\"=\"*80)\n",
    "organize_for_sololearn(\"./MIMIC-CXR/train\", \"./MIMIC-CXR-organized/train\", \"xray\")\n",
    "organize_for_sololearn(\"./MIMIC-CXR/val\", \"./MIMIC-CXR-organized/val\", \"xray\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Processing TRAIN set ===\n",
      "Scanning for images...\n",
      "Found 223414 images total.\n",
      "Starting to link 223414 images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Linking: 100%|██████████| 223414/223414 [01:27<00:00, 2559.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Linked 0/223414 images successfully.\n",
      "Failed links: 223414\n",
      "First 5 errors: [('view1_frontal.jpg', \"[WinError 183] Cannot create a file when that file already exists: 'CheXpert\\\\\\\\train\\\\\\\\patient00001\\\\\\\\study1\\\\\\\\view1_frontal.jpg' -> 'chexpert-organized\\\\\\\\train\\\\\\\\xray\\\\\\\\train_patient00001_study1_view1_frontal.jpg'\"), ('view1_frontal.jpg', \"[WinError 183] Cannot create a file when that file already exists: 'CheXpert\\\\\\\\train\\\\\\\\patient00002\\\\\\\\study1\\\\\\\\view1_frontal.jpg' -> 'chexpert-organized\\\\\\\\train\\\\\\\\xray\\\\\\\\train_patient00002_study1_view1_frontal.jpg'\"), ('view2_lateral.jpg', \"[WinError 183] Cannot create a file when that file already exists: 'CheXpert\\\\\\\\train\\\\\\\\patient00002\\\\\\\\study1\\\\\\\\view2_lateral.jpg' -> 'chexpert-organized\\\\\\\\train\\\\\\\\xray\\\\\\\\train_patient00002_study1_view2_lateral.jpg'\"), ('view1_frontal.jpg', \"[WinError 183] Cannot create a file when that file already exists: 'CheXpert\\\\\\\\train\\\\\\\\patient00002\\\\\\\\study2\\\\\\\\view1_frontal.jpg' -> 'chexpert-organized\\\\\\\\train\\\\\\\\xray\\\\\\\\train_patient00002_study2_view1_frontal.jpg'\"), ('view1_frontal.jpg', \"[WinError 183] Cannot create a file when that file already exists: 'CheXpert\\\\\\\\train\\\\\\\\patient00003\\\\\\\\study1\\\\\\\\view1_frontal.jpg' -> 'chexpert-organized\\\\\\\\train\\\\\\\\xray\\\\\\\\train_patient00003_study1_view1_frontal.jpg'\")]\n",
      "Done! Train images are now in: chexpert-organized/train/xray/\n",
      "\n",
      "=== Processing VALIDATION set ===\n",
      "Scanning for images...\n",
      "Found 234 images total.\n",
      "Starting to link 234 images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Linking: 100%|██████████| 234/234 [00:00<00:00, 2818.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Linked 0/234 images successfully.\n",
      "Failed links: 234\n",
      "First 5 errors: [('view1_frontal.jpg', \"[WinError 183] Cannot create a file when that file already exists: 'CheXpert\\\\\\\\val\\\\\\\\patient64541\\\\\\\\study1\\\\\\\\view1_frontal.jpg' -> 'chexpert-organized\\\\\\\\val\\\\\\\\xray\\\\\\\\val_patient64541_study1_view1_frontal.jpg'\"), ('view1_frontal.jpg', \"[WinError 183] Cannot create a file when that file already exists: 'CheXpert\\\\\\\\val\\\\\\\\patient64542\\\\\\\\study1\\\\\\\\view1_frontal.jpg' -> 'chexpert-organized\\\\\\\\val\\\\\\\\xray\\\\\\\\val_patient64542_study1_view1_frontal.jpg'\"), ('view2_lateral.jpg', \"[WinError 183] Cannot create a file when that file already exists: 'CheXpert\\\\\\\\val\\\\\\\\patient64542\\\\\\\\study1\\\\\\\\view2_lateral.jpg' -> 'chexpert-organized\\\\\\\\val\\\\\\\\xray\\\\\\\\val_patient64542_study1_view2_lateral.jpg'\"), ('view1_frontal.jpg', \"[WinError 183] Cannot create a file when that file already exists: 'CheXpert\\\\\\\\val\\\\\\\\patient64543\\\\\\\\study1\\\\\\\\view1_frontal.jpg' -> 'chexpert-organized\\\\\\\\val\\\\\\\\xray\\\\\\\\val_patient64543_study1_view1_frontal.jpg'\"), ('view1_frontal.jpg', \"[WinError 183] Cannot create a file when that file already exists: 'CheXpert\\\\\\\\val\\\\\\\\patient64544\\\\\\\\study1\\\\\\\\view1_frontal.jpg' -> 'chexpert-organized\\\\\\\\val\\\\\\\\xray\\\\\\\\val_patient64544_study1_view1_frontal.jpg'\")]\n",
      "Done! Val images are now in: chexpert-organized/val/xray/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "def count_images(flat_dir):\n",
    "    \"\"\"Count images in directory without copying\"\"\"\n",
    "    flat_path = Path(flat_dir)\n",
    "    print(\"Scanning for images...\")\n",
    "    image_files = [\n",
    "        f for f in flat_path.rglob(\"*\")\n",
    "        if f.is_file() and f.suffix.lower() in (\".jpg\", \".jpeg\", \".png\")\n",
    "    ]\n",
    "    print(f\"Found {len(image_files)} images total.\")\n",
    "    return image_files\n",
    "\n",
    "def copy_images_simple(image_files, output_dir, class_name=\"images\"):\n",
    "    \"\"\"Link images one at a time with progress bar (much faster than copying)\"\"\"\n",
    "    class_dir = Path(output_dir) / class_name\n",
    "    class_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    print(f\"Starting to link {len(image_files)} images...\")\n",
    "    successful = 0\n",
    "    failed = []\n",
    "    \n",
    "    for img_file in tqdm(image_files, desc=\"Linking\"):\n",
    "        try:\n",
    "            # Create unique filename from full path\n",
    "            # e.g., patient00001/study1/view1_frontal.jpg -> patient00001_study1_view1_frontal.jpg\n",
    "            relative_path = img_file.relative_to(img_file.parents[3])  # Get path from train/val root\n",
    "            unique_name = str(relative_path).replace(os.sep, '_')\n",
    "            \n",
    "            dst = class_dir / unique_name\n",
    "            \n",
    "            # Use hard link instead of copy (instantaneous, no disk space used)\n",
    "            os.link(img_file, dst)\n",
    "            successful += 1\n",
    "            \n",
    "        except Exception as e:\n",
    "            failed.append((img_file.name, str(e)))\n",
    "    \n",
    "    print(f\"\\nLinked {successful}/{len(image_files)} images successfully.\")\n",
    "    if failed:\n",
    "        print(f\"Failed links: {len(failed)}\")\n",
    "        print(\"First 5 errors:\", failed[:5])\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # TRAIN SET\n",
    "    print(\"\\n=== Processing TRAIN set ===\")\n",
    "    train_images = count_images(\"./CheXpert/train\")\n",
    "    copy_images_simple(train_images, \"chexpert-organized/train\", class_name=\"xray\")\n",
    "    print(\"Done! Train images are now in: chexpert-organized/train/xray/\")\n",
    "    \n",
    "    # VALIDATION SET\n",
    "    print(\"\\n=== Processing VALIDATION set ===\")\n",
    "    val_images = count_images(\"./CheXpert/val\")\n",
    "    copy_images_simple(val_images, \"chexpert-organized/val\", class_name=\"xray\")\n",
    "    print(\"Done! Val images are now in: chexpert-organized/val/xray/\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-17T23:06:04.059255Z",
     "start_time": "2025-11-17T23:06:04.055880Z"
    }
   },
   "outputs": [],
   "source": [
    "df_train_chexpert.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-17T23:06:08.544149Z",
     "start_time": "2025-11-17T23:06:08.540252Z"
    }
   },
   "outputs": [],
   "source": [
    "df_train_mimic.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training with solo learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: solo-learn 1.0.6\n",
      "Uninstalling solo-learn-1.0.6:\n",
      "  Successfully uninstalled solo-learn-1.0.6\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting git+https://github.com/vturrisi/solo-learn.git@main\n",
      "  Cloning https://github.com/vturrisi/solo-learn.git (to revision main) to c:\\users\\joseh\\appdata\\local\\temp\\pip-req-build-ki8uwgp2\n",
      "  Resolved https://github.com/vturrisi/solo-learn.git to commit b69b4bd27472593919956d9ac58902a301537a4d\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: torch>=1.10.0 in c:\\users\\joseh\\appdata\\roaming\\python\\python313\\site-packages (from solo-learn==1.0.6) (2.6.0+cu124)\n",
      "Requirement already satisfied: torchvision>=0.11.1 in c:\\users\\joseh\\appdata\\roaming\\python\\python313\\site-packages (from solo-learn==1.0.6) (0.21.0+cu124)\n",
      "Requirement already satisfied: einops in c:\\users\\joseh\\appdata\\roaming\\python\\python313\\site-packages (from solo-learn==1.0.6) (0.8.1)\n",
      "Requirement already satisfied: lightning==2.1.2 in c:\\users\\joseh\\appdata\\roaming\\python\\python313\\site-packages (from solo-learn==1.0.6) (2.1.2)\n",
      "Requirement already satisfied: torchmetrics<0.12.0,>=0.6.0 in c:\\users\\joseh\\appdata\\roaming\\python\\python313\\site-packages (from solo-learn==1.0.6) (0.11.4)\n",
      "Requirement already satisfied: tqdm in c:\\users\\joseh\\appdata\\roaming\\python\\python313\\site-packages (from solo-learn==1.0.6) (4.67.1)\n",
      "Requirement already satisfied: wandb in c:\\users\\joseh\\appdata\\roaming\\python\\python313\\site-packages (from solo-learn==1.0.6) (0.23.0)\n",
      "Requirement already satisfied: scipy in c:\\users\\joseh\\appdata\\roaming\\python\\python313\\site-packages (from solo-learn==1.0.6) (1.16.3)\n",
      "Requirement already satisfied: timm in c:\\users\\joseh\\appdata\\roaming\\python\\python313\\site-packages (from solo-learn==1.0.6) (1.0.22)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\joseh\\appdata\\roaming\\python\\python313\\site-packages (from solo-learn==1.0.6) (1.7.2)\n",
      "Requirement already satisfied: hydra-core in c:\\users\\joseh\\appdata\\roaming\\python\\python313\\site-packages (from solo-learn==1.0.6) (1.3.2)\n",
      "Requirement already satisfied: PyYAML<8.0,>=5.4 in c:\\users\\joseh\\appdata\\roaming\\python\\python313\\site-packages (from lightning==2.1.2->solo-learn==1.0.6) (6.0.3)\n",
      "Requirement already satisfied: fsspec<2025.0,>2021.06.0 in c:\\users\\joseh\\appdata\\roaming\\python\\python313\\site-packages (from fsspec[http]<2025.0,>2021.06.0->lightning==2.1.2->solo-learn==1.0.6) (2024.12.0)\n",
      "Requirement already satisfied: lightning-utilities<2.0,>=0.8.0 in c:\\users\\joseh\\appdata\\roaming\\python\\python313\\site-packages (from lightning==2.1.2->solo-learn==1.0.6) (0.15.2)\n",
      "Requirement already satisfied: numpy<3.0,>=1.17.2 in c:\\users\\joseh\\appdata\\roaming\\python\\python313\\site-packages (from lightning==2.1.2->solo-learn==1.0.6) (2.3.3)\n",
      "Requirement already satisfied: packaging<25.0,>=20.0 in c:\\users\\joseh\\appdata\\roaming\\python\\python313\\site-packages (from lightning==2.1.2->solo-learn==1.0.6) (24.2)\n",
      "Requirement already satisfied: typing-extensions<6.0,>=4.0.0 in c:\\users\\joseh\\appdata\\roaming\\python\\python313\\site-packages (from lightning==2.1.2->solo-learn==1.0.6) (4.13.0)\n",
      "Requirement already satisfied: pytorch-lightning in c:\\users\\joseh\\appdata\\roaming\\python\\python313\\site-packages (from lightning==2.1.2->solo-learn==1.0.6) (2.5.6)\n",
      "Requirement already satisfied: filelock in c:\\users\\joseh\\appdata\\roaming\\python\\python313\\site-packages (from torch>=1.10.0->solo-learn==1.0.6) (3.20.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\joseh\\appdata\\roaming\\python\\python313\\site-packages (from torch>=1.10.0->solo-learn==1.0.6) (3.6)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\joseh\\appdata\\roaming\\python\\python313\\site-packages (from torch>=1.10.0->solo-learn==1.0.6) (3.1.6)\n",
      "Requirement already satisfied: setuptools in c:\\users\\joseh\\appdata\\roaming\\python\\python313\\site-packages (from torch>=1.10.0->solo-learn==1.0.6) (80.9.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\joseh\\appdata\\roaming\\python\\python313\\site-packages (from torch>=1.10.0->solo-learn==1.0.6) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\joseh\\appdata\\roaming\\python\\python313\\site-packages (from sympy==1.13.1->torch>=1.10.0->solo-learn==1.0.6) (1.3.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\joseh\\appdata\\roaming\\python\\python313\\site-packages (from torchvision>=0.11.1->solo-learn==1.0.6) (12.0.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\joseh\\appdata\\roaming\\python\\python313\\site-packages (from tqdm->solo-learn==1.0.6) (0.4.6)\n",
      "Requirement already satisfied: omegaconf<2.4,>=2.2 in c:\\users\\joseh\\appdata\\roaming\\python\\python313\\site-packages (from hydra-core->solo-learn==1.0.6) (2.3.0)\n",
      "Requirement already satisfied: antlr4-python3-runtime==4.9.* in c:\\users\\joseh\\appdata\\roaming\\python\\python313\\site-packages (from hydra-core->solo-learn==1.0.6) (4.9.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\joseh\\appdata\\roaming\\python\\python313\\site-packages (from scikit-learn->solo-learn==1.0.6) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\joseh\\appdata\\roaming\\python\\python313\\site-packages (from scikit-learn->solo-learn==1.0.6) (3.6.0)\n",
      "Requirement already satisfied: huggingface_hub in c:\\users\\joseh\\appdata\\roaming\\python\\python313\\site-packages (from timm->solo-learn==1.0.6) (1.1.5)\n",
      "Requirement already satisfied: safetensors in c:\\users\\joseh\\appdata\\roaming\\python\\python313\\site-packages (from timm->solo-learn==1.0.6) (0.7.0)\n",
      "Requirement already satisfied: click>=8.0.1 in c:\\users\\joseh\\appdata\\roaming\\python\\python313\\site-packages (from wandb->solo-learn==1.0.6) (8.3.1)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in c:\\users\\joseh\\appdata\\roaming\\python\\python313\\site-packages (from wandb->solo-learn==1.0.6) (3.1.45)\n",
      "Requirement already satisfied: platformdirs in c:\\users\\joseh\\appdata\\roaming\\python\\python313\\site-packages (from wandb->solo-learn==1.0.6) (4.5.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in c:\\users\\joseh\\appdata\\roaming\\python\\python313\\site-packages (from wandb->solo-learn==1.0.6) (6.33.1)\n",
      "Requirement already satisfied: pydantic<3 in c:\\users\\joseh\\appdata\\roaming\\python\\python313\\site-packages (from wandb->solo-learn==1.0.6) (2.11.0)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in c:\\users\\joseh\\appdata\\roaming\\python\\python313\\site-packages (from wandb->solo-learn==1.0.6) (2.32.5)\n",
      "Requirement already satisfied: sentry-sdk>=2.0.0 in c:\\users\\joseh\\appdata\\roaming\\python\\python313\\site-packages (from wandb->solo-learn==1.0.6) (2.45.0)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in c:\\users\\joseh\\appdata\\roaming\\python\\python313\\site-packages (from fsspec[http]<2025.0,>2021.06.0->lightning==2.1.2->solo-learn==1.0.6) (3.13.2)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in c:\\users\\joseh\\appdata\\roaming\\python\\python313\\site-packages (from gitpython!=3.1.29,>=1.0.0->wandb->solo-learn==1.0.6) (4.0.12)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\joseh\\appdata\\roaming\\python\\python313\\site-packages (from pydantic<3->wandb->solo-learn==1.0.6) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.0 in c:\\users\\joseh\\appdata\\roaming\\python\\python313\\site-packages (from pydantic<3->wandb->solo-learn==1.0.6) (2.33.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\joseh\\appdata\\roaming\\python\\python313\\site-packages (from pydantic<3->wandb->solo-learn==1.0.6) (0.4.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\joseh\\appdata\\roaming\\python\\python313\\site-packages (from requests<3,>=2.0.0->wandb->solo-learn==1.0.6) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\joseh\\appdata\\roaming\\python\\python313\\site-packages (from requests<3,>=2.0.0->wandb->solo-learn==1.0.6) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\joseh\\appdata\\roaming\\python\\python313\\site-packages (from requests<3,>=2.0.0->wandb->solo-learn==1.0.6) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\joseh\\appdata\\roaming\\python\\python313\\site-packages (from requests<3,>=2.0.0->wandb->solo-learn==1.0.6) (2025.1.31)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in c:\\users\\joseh\\appdata\\roaming\\python\\python313\\site-packages (from huggingface_hub->timm->solo-learn==1.0.6) (1.2.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\joseh\\appdata\\roaming\\python\\python313\\site-packages (from huggingface_hub->timm->solo-learn==1.0.6) (0.28.1)\n",
      "Requirement already satisfied: shellingham in c:\\users\\joseh\\appdata\\roaming\\python\\python313\\site-packages (from huggingface_hub->timm->solo-learn==1.0.6) (1.5.4)\n",
      "Requirement already satisfied: typer-slim in c:\\users\\joseh\\appdata\\roaming\\python\\python313\\site-packages (from huggingface_hub->timm->solo-learn==1.0.6) (0.20.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\joseh\\appdata\\roaming\\python\\python313\\site-packages (from jinja2->torch>=1.10.0->solo-learn==1.0.6) (3.0.3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in c:\\users\\joseh\\appdata\\roaming\\python\\python313\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>2021.06.0->lightning==2.1.2->solo-learn==1.0.6) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in c:\\users\\joseh\\appdata\\roaming\\python\\python313\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>2021.06.0->lightning==2.1.2->solo-learn==1.0.6) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\joseh\\appdata\\roaming\\python\\python313\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>2021.06.0->lightning==2.1.2->solo-learn==1.0.6) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\joseh\\appdata\\roaming\\python\\python313\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>2021.06.0->lightning==2.1.2->solo-learn==1.0.6) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\joseh\\appdata\\roaming\\python\\python313\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>2021.06.0->lightning==2.1.2->solo-learn==1.0.6) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\joseh\\appdata\\roaming\\python\\python313\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>2021.06.0->lightning==2.1.2->solo-learn==1.0.6) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\joseh\\appdata\\roaming\\python\\python313\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>2021.06.0->lightning==2.1.2->solo-learn==1.0.6) (1.22.0)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in c:\\users\\joseh\\appdata\\roaming\\python\\python313\\site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb->solo-learn==1.0.6) (5.0.2)\n",
      "Requirement already satisfied: anyio in c:\\users\\joseh\\appdata\\roaming\\python\\python313\\site-packages (from httpx<1,>=0.23.0->huggingface_hub->timm->solo-learn==1.0.6) (4.9.0)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\joseh\\appdata\\roaming\\python\\python313\\site-packages (from httpx<1,>=0.23.0->huggingface_hub->timm->solo-learn==1.0.6) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\joseh\\appdata\\roaming\\python\\python313\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->huggingface_hub->timm->solo-learn==1.0.6) (0.14.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\joseh\\appdata\\roaming\\python\\python313\\site-packages (from anyio->httpx<1,>=0.23.0->huggingface_hub->timm->solo-learn==1.0.6) (1.3.1)\n",
      "Building wheels for collected packages: solo-learn\n",
      "  Building wheel for solo-learn (pyproject.toml): started\n",
      "  Building wheel for solo-learn (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for solo-learn: filename=solo_learn-1.0.6-py3-none-any.whl size=247918 sha256=558ff2ea373ee6d6a99ae5fd2a4d5902a02e7819e760b0cd4bd0b78b53790c70\n",
      "  Stored in directory: C:\\Users\\joseh\\AppData\\Local\\Temp\\pip-ephem-wheel-cache-t4issle_\\wheels\\2a\\5a\\52\\5612c237b07b5e53621c701e9732f47129a691f7c0450f6269\n",
      "Successfully built solo-learn\n",
      "Installing collected packages: solo-learn\n",
      "Successfully installed solo-learn-1.0.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Running command git clone --filter=blob:none --quiet https://github.com/vturrisi/solo-learn.git 'C:\\Users\\joseh\\AppData\\Local\\Temp\\pip-req-build-ki8uwgp2'\n",
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.3\n",
      "[notice] To update, run: C:\\Python313\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall solo-learn -y\n",
    "!pip install git+https://github.com/vturrisi/solo-learn.git@main\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import solo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "simclr module found\n"
     ]
    }
   ],
   "source": [
    "from solo.methods import simclr\n",
    "print(\"simclr module found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'types.SimpleNamespace' object has no attribute '_is_none'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\joseh\\.conda\\envs\\py312\\Lib\\site-packages\\omegaconf\\omegaconf.py:674\u001b[39m, in \u001b[36mOmegaConf.select\u001b[39m\u001b[34m(cfg, key, default, throw_on_resolution_failure, throw_on_missing)\u001b[39m\n\u001b[32m    673\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m674\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mselect_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    675\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    676\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    677\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdefault\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdefault\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    678\u001b[39m \u001b[43m        \u001b[49m\u001b[43mthrow_on_resolution_failure\u001b[49m\u001b[43m=\u001b[49m\u001b[43mthrow_on_resolution_failure\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    679\u001b[39m \u001b[43m        \u001b[49m\u001b[43mthrow_on_missing\u001b[49m\u001b[43m=\u001b[49m\u001b[43mthrow_on_missing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    680\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    681\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\joseh\\.conda\\envs\\py312\\Lib\\site-packages\\omegaconf\\_impl.py:58\u001b[39m, in \u001b[36mselect_value\u001b[39m\u001b[34m(cfg, key, default, throw_on_resolution_failure, throw_on_missing, absolute_key)\u001b[39m\n\u001b[32m     49\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mselect_value\u001b[39m(\n\u001b[32m     50\u001b[39m     cfg: Container,\n\u001b[32m     51\u001b[39m     key: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     56\u001b[39m     absolute_key: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m     57\u001b[39m ) -> Any:\n\u001b[32m---> \u001b[39m\u001b[32m58\u001b[39m     node = \u001b[43mselect_node\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     59\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     60\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     61\u001b[39m \u001b[43m        \u001b[49m\u001b[43mthrow_on_resolution_failure\u001b[49m\u001b[43m=\u001b[49m\u001b[43mthrow_on_resolution_failure\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     62\u001b[39m \u001b[43m        \u001b[49m\u001b[43mthrow_on_missing\u001b[49m\u001b[43m=\u001b[49m\u001b[43mthrow_on_missing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     63\u001b[39m \u001b[43m        \u001b[49m\u001b[43mabsolute_key\u001b[49m\u001b[43m=\u001b[49m\u001b[43mabsolute_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     64\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     66\u001b[39m     node_not_found = node \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\joseh\\.conda\\envs\\py312\\Lib\\site-packages\\omegaconf\\_impl.py:92\u001b[39m, in \u001b[36mselect_node\u001b[39m\u001b[34m(cfg, key, throw_on_resolution_failure, throw_on_missing, absolute_key)\u001b[39m\n\u001b[32m     90\u001b[39m     key = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m92\u001b[39m cfg, key = \u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_resolve_key_and_root\u001b[49m(key)\n\u001b[32m     93\u001b[39m _root, _last_key, node = cfg._select_impl(\n\u001b[32m     94\u001b[39m     key,\n\u001b[32m     95\u001b[39m     throw_on_missing=throw_on_missing,\n\u001b[32m     96\u001b[39m     throw_on_resolution_failure=throw_on_resolution_failure,\n\u001b[32m     97\u001b[39m )\n",
      "\u001b[31mAttributeError\u001b[39m: 'types.SimpleNamespace' object has no attribute '_resolve_key_and_root'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 52\u001b[39m\n\u001b[32m     46\u001b[39m seed_everything(cfg.seed)\n\u001b[32m     48\u001b[39m \u001b[38;5;66;03m# ------------------------------\u001b[39;00m\n\u001b[32m     49\u001b[39m \u001b[38;5;66;03m# 3️⃣ Initialize SimCLR\u001b[39;00m\n\u001b[32m     50\u001b[39m \u001b[38;5;66;03m# ------------------------------\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m model = \u001b[43mMETHODS\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msimclr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     53\u001b[39m make_contiguous(model)\n\u001b[32m     55\u001b[39m \u001b[38;5;66;03m# ------------------------------\u001b[39;00m\n\u001b[32m     56\u001b[39m \u001b[38;5;66;03m# 4️⃣ Create PyTorch Lightning Trainer\u001b[39;00m\n\u001b[32m     57\u001b[39m \u001b[38;5;66;03m# ------------------------------\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\joseh\\.conda\\envs\\py312\\Lib\\site-packages\\solo\\methods\\simclr.py:40\u001b[39m, in \u001b[36mSimCLR.__init__\u001b[39m\u001b[34m(self, cfg)\u001b[39m\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, cfg: omegaconf.DictConfig):\n\u001b[32m     31\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Implements SimCLR (https://arxiv.org/abs/2002.05709).\u001b[39;00m\n\u001b[32m     32\u001b[39m \n\u001b[32m     33\u001b[39m \u001b[33;03m    Extra cfg settings:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     37\u001b[39m \u001b[33;03m            temperature (float): temperature for the softmax in the contrastive loss.\u001b[39;00m\n\u001b[32m     38\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     42\u001b[39m     \u001b[38;5;28mself\u001b[39m.temperature: \u001b[38;5;28mfloat\u001b[39m = cfg.method_kwargs.temperature\n\u001b[32m     44\u001b[39m     proj_hidden_dim: \u001b[38;5;28mint\u001b[39m = cfg.method_kwargs.proj_hidden_dim\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\joseh\\.conda\\envs\\py312\\Lib\\site-packages\\solo\\methods\\base.py:179\u001b[39m, in \u001b[36mBaseMethod.__init__\u001b[39m\u001b[34m(self, cfg)\u001b[39m\n\u001b[32m    176\u001b[39m \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m()\n\u001b[32m    178\u001b[39m \u001b[38;5;66;03m# add default values and assert that config has the basic needed settings\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m179\u001b[39m cfg = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43madd_and_assert_specific_cfg\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    181\u001b[39m \u001b[38;5;28mself\u001b[39m.cfg: omegaconf.DictConfig = cfg\n\u001b[32m    183\u001b[39m \u001b[38;5;66;03m##############################\u001b[39;00m\n\u001b[32m    184\u001b[39m \u001b[38;5;66;03m# Backbone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\joseh\\.conda\\envs\\py312\\Lib\\site-packages\\solo\\methods\\simclr.py:65\u001b[39m, in \u001b[36mSimCLR.add_and_assert_specific_cfg\u001b[39m\u001b[34m(cfg)\u001b[39m\n\u001b[32m     54\u001b[39m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34madd_and_assert_specific_cfg\u001b[39m(cfg: omegaconf.DictConfig) -> omegaconf.DictConfig:\n\u001b[32m     56\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Adds method specific default values/checks for config.\u001b[39;00m\n\u001b[32m     57\u001b[39m \n\u001b[32m     58\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     62\u001b[39m \u001b[33;03m        omegaconf.DictConfig: same as the argument, used to avoid errors.\u001b[39;00m\n\u001b[32m     63\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m65\u001b[39m     cfg = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mSimCLR\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mSimCLR\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43madd_and_assert_specific_cfg\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     67\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m omegaconf.OmegaConf.is_missing(cfg, \u001b[33m\"\u001b[39m\u001b[33mmethod_kwargs.proj_output_dim\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     68\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m omegaconf.OmegaConf.is_missing(cfg, \u001b[33m\"\u001b[39m\u001b[33mmethod_kwargs.proj_hidden_dim\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\joseh\\.conda\\envs\\py312\\Lib\\site-packages\\solo\\methods\\base.py:277\u001b[39m, in \u001b[36mBaseMethod.add_and_assert_specific_cfg\u001b[39m\u001b[34m(cfg)\u001b[39m\n\u001b[32m    267\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Adds method specific default values/checks for config.\u001b[39;00m\n\u001b[32m    268\u001b[39m \n\u001b[32m    269\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    273\u001b[39m \u001b[33;03m    omegaconf.DictConfig: same as the argument, used to avoid errors.\u001b[39;00m\n\u001b[32m    274\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    276\u001b[39m \u001b[38;5;66;03m# default for extra backbone kwargs (use pytorch's default if not available)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m277\u001b[39m cfg.backbone.kwargs = \u001b[43momegaconf_select\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mbackbone.kwargs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    279\u001b[39m \u001b[38;5;66;03m# default parameters for optimizer\u001b[39;00m\n\u001b[32m    280\u001b[39m cfg.optimizer.exclude_bias_n_norm_wd = omegaconf_select(\n\u001b[32m    281\u001b[39m     cfg, \u001b[33m\"\u001b[39m\u001b[33moptimizer.exclude_bias_n_norm_wd\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    282\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\joseh\\.conda\\envs\\py312\\Lib\\site-packages\\solo\\utils\\misc.py:450\u001b[39m, in \u001b[36momegaconf_select\u001b[39m\u001b[34m(cfg, key, default)\u001b[39m\n\u001b[32m    448\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34momegaconf_select\u001b[39m(cfg, key, default=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    449\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Wrapper for OmegaConf.select to allow None to be returned instead of 'None'.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m450\u001b[39m     value = \u001b[43mOmegaConf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdefault\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdefault\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    451\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m value == \u001b[33m\"\u001b[39m\u001b[33mNone\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    452\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\joseh\\.conda\\envs\\py312\\Lib\\site-packages\\omegaconf\\omegaconf.py:682\u001b[39m, in \u001b[36mOmegaConf.select\u001b[39m\u001b[34m(cfg, key, default, throw_on_resolution_failure, throw_on_missing)\u001b[39m\n\u001b[32m    674\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m select_value(\n\u001b[32m    675\u001b[39m         cfg=cfg,\n\u001b[32m    676\u001b[39m         key=key,\n\u001b[32m   (...)\u001b[39m\u001b[32m    679\u001b[39m         throw_on_missing=throw_on_missing,\n\u001b[32m    680\u001b[39m     )\n\u001b[32m    681\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m--> \u001b[39m\u001b[32m682\u001b[39m     \u001b[43mformat_and_raise\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcause\u001b[49m\u001b[43m=\u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\joseh\\.conda\\envs\\py312\\Lib\\site-packages\\omegaconf\\_utils.py:833\u001b[39m, in \u001b[36mformat_and_raise\u001b[39m\u001b[34m(node, key, value, msg, cause, type_override)\u001b[39m\n\u001b[32m    831\u001b[39m     ref_type_str = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    832\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m833\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mnode\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_is_none\u001b[49m():\n\u001b[32m    834\u001b[39m         child_node = node._get_node(key, validate_access=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    836\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[31mAttributeError\u001b[39m: 'types.SimpleNamespace' object has no attribute '_is_none'"
     ]
    }
   ],
   "source": [
    "# ------------------------------\n",
    "# Jupyter Notebook: SimCLR Sanity Test\n",
    "# ------------------------------\n",
    "\n",
    "import os\n",
    "from types import SimpleNamespace\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from lightning.pytorch import Trainer, seed_everything\n",
    "from lightning.pytorch.callbacks import LearningRateMonitor\n",
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "from solo.methods import METHODS\n",
    "from solo.utils.misc import make_contiguous\n",
    "\n",
    "\n",
    "\n",
    "cfg = SimpleNamespace(\n",
    "    seed=42,\n",
    "    method=\"simclr\",\n",
    "    data=SimpleNamespace(\n",
    "        dataset=\"custom\",\n",
    "        train_path=\"Chexpert-organized/train\",\n",
    "        val_path=\"CheXpert-organized/val\",\n",
    "        format=\"image_folder\",\n",
    "        num_large_crops=2,\n",
    "        num_small_crops=0,\n",
    "        no_labels=True,\n",
    "        num_workers=0,\n",
    "    ),\n",
    "    optimizer=SimpleNamespace(\n",
    "        batch_size=2,\n",
    "        optimizer=\"sgd\",\n",
    "        lr=0.01,\n",
    "        weight_decay=1e-6,\n",
    "    ),\n",
    "    max_epochs=2,\n",
    "    name=\"chexpert_test\",\n",
    "    gpus=1 if torch.cuda.is_available() else 0,\n",
    "    wandb=SimpleNamespace(enabled=False),\n",
    "    performance=SimpleNamespace(disable_channel_last=True),\n",
    ")\n",
    "\n",
    "\n",
    "seed_everything(cfg.seed)\n",
    "\n",
    "# ------------------------------\n",
    "# 3️⃣ Initialize SimCLR\n",
    "# ------------------------------\n",
    "\n",
    "model = METHODS[\"simclr\"](cfg)\n",
    "make_contiguous(model)\n",
    "\n",
    "# ------------------------------\n",
    "# 4️⃣ Create PyTorch Lightning Trainer\n",
    "# ------------------------------\n",
    "\n",
    "logger = TensorBoardLogger(\"logs\", name=cfg.name)\n",
    "lr_monitor = LearningRateMonitor(logging_interval=\"step\")\n",
    "\n",
    "trainer = Trainer(\n",
    "    max_epochs=cfg.max_epochs,\n",
    "    accelerator=\"gpu\" if cfg.gpus else \"cpu\",\n",
    "    devices=1 if cfg.gpus else None,\n",
    "    logger=logger,\n",
    "    callbacks=[lr_monitor],\n",
    ")\n",
    "\n",
    "# ------------------------------\n",
    "# 5️⃣ Create dataloaders\n",
    "# ------------------------------\n",
    "\n",
    "from solo.data.pretrain_dataloader import prepare_datasets, prepare_dataloader, NCropAugmentation, FullTransformPipeline, build_transform_pipeline\n",
    "\n",
    "# Minimal transforms for sanity test\n",
    "transform = [build_transform_pipeline(\"imagenet100\", {\"brightness\":0.4,\"contrast\":0.4,\"saturation\":0.2,\"hue\":0.1})]\n",
    "transform = FullTransformPipeline([NCropAugmentation(transform[0], 2)])\n",
    "\n",
    "train_dataset = prepare_datasets(\"custom\", transform, train_data_path=str(train_dir.parent), no_labels=True)\n",
    "train_loader = prepare_dataloader(train_dataset, batch_size=cfg.optimizer.batch_size, num_workers=0)\n",
    "\n",
    "val_dataset = prepare_datasets(\"custom\", transform, train_data_path=str(val_dir.parent), no_labels=True)\n",
    "val_loader = prepare_dataloader(val_dataset, batch_size=cfg.optimizer.batch_size, num_workers=0)\n",
    "\n",
    "# ------------------------------\n",
    "# 6️⃣ Start training\n",
    "# ------------------------------\n",
    "\n",
    "trainer.fit(model, train_loader, val_loader)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3234304305.py, line 2)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mpython -m solo.main_pretrain \\\u001b[39m\n              ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# CheXpert\n",
    "!python -m solo.main_pretrain \\\n",
    "  --method simclr \\\n",
    "  --dataset custom \\\n",
    "  --data_dir /path/to/chesXpert \\\n",
    "  --train_dir train \\\n",
    "  --val_dir val \\\n",
    "  --encoder resnet50 \\\n",
    "  --batch_size 128 \\\n",
    "  --max_epochs 50 \\\n",
    "  --num_workers 8 \\\n",
    "  --gpus 1 \\\n",
    "  --precision 16 \\\n",
    "  --logger tensorboard \\\n",
    "  --log_dir ./logs \\\n",
    "  --project mimic_ssl \\\n",
    "  --name simclr_chexpert\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embedding extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CheXpert\n",
    "python -m solo.main_linear \\\n",
    "  --backbone resnet50 \\\n",
    "  --pretrained_feature_extractor ./checkpoints/simclr_chexpert.ckpt \\\n",
    "  --extract_features \\\n",
    "  --data_dir /path/to/CheXpert \\\n",
    "  --train_dir train \\\n",
    "  --output_features simclr_chexpert_embeddings.pt\n",
    "\n",
    "# MIMIC\n",
    "python -m solo.main_linear \\\n",
    "  --backbone resnet50 \\\n",
    "  --pretrained_feature_extractor ./checkpoints/simclr_mimic.ckpt \\\n",
    "  --extract_features \\\n",
    "  --data_dir /path/to/MIMIC-CXR \\\n",
    "  --train_dir train \\\n",
    "  --output_features simclr_mimic_embeddings.pt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Load embeddings\n",
    "chexpert_emb = torch.load(\"./embeddings/simclr_chexpert_embeddings.pt\")\n",
    "mimic_emb = torch.load(\"./embeddings/simclr_mimic_embeddings.pt\")\n",
    "\n",
    "# Procrustes alignment\n",
    "def procrustes_alignment(X, Y):\n",
    "    U, _, Vt = torch.linalg.svd(Y.T @ X)\n",
    "    R = U @ Vt\n",
    "    return X @ R.T, R\n",
    "\n",
    "chexpert_aligned, R = procrustes_alignment(chexpert_emb, mimic_emb)\n",
    "torch.save(chexpert_aligned, \"./embeddings/chexpert_aligned.pt\")\n",
    "\n",
    "# Example: subgroup distance with Polars\n",
    "def subgroup_centroid_distance(embeddings, demographics_col):\n",
    "    groups = demographics_col.unique().to_list()\n",
    "    centroids = {}\n",
    "    for g in groups:\n",
    "        mask = demographics_col == g\n",
    "        emb_group = embeddings[mask.to_numpy()]\n",
    "        centroids[g] = emb_group.mean(dim=0)\n",
    "    dist = {}\n",
    "    for g1 in groups:\n",
    "        for g2 in groups:\n",
    "            dist[(g1, g2)] = torch.norm(centroids[g1] - centroids[g2]).item()\n",
    "    return dist\n",
    "\n",
    "demographics = df_train_chexpert[\"Sex\"]  # or \"Gender\" column\n",
    "dists = subgroup_centroid_distance(chexpert_aligned, demographics)\n",
    "print(dists)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir ./logs\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPk3VYQAvkG8XZbveKPAqcC",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.12",
   "language": "python",
   "name": "py312"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  },
  "polyglot_notebook": {
   "kernelInfo": {
    "defaultKernelName": "csharp",
    "items": [
     {
      "aliases": [],
      "name": "csharp"
     }
    ]
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
