{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/ccorbett0116/Fall2025ResearchProject/blob/main/Research_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZVMwQDnKO8XY"
   },
   "source": [
    "# Project Title:\n",
    "# Authors: Jose Henriquez, Cole Corbett\n",
    "## Description:\n",
    "The deployment of medical AI systems across different hospitals raises critical questions about whether fairness and representation quality can be reliably transferred across clinical domains. Models trained on one hospital’s imaging data are often reused in new environments where patient demographics, imaging devices, and diagnostic practices differ substantially, potentially resulting in unintended bias against certain groups. This project investigates this challenge by studying fairness-aware representation alignment in medical imaging. The student will train contrastive learning models—such as SimCLR—independently on two large-scale chest X-ray datasets: CheXpert (from Stanford Hospital) and MIMIC-CXR (from Beth Israel Deaconess Medical Center). After learning embeddings in each domain, the student will apply domain alignment techniques such as Procrustes alignment to map representations from the CheXpert embedding space into the MIMIC-CXR space. The aligned embeddings will then be evaluated using fairness metrics designed for representation spaces, including demographic subgroup alignment, intra- vs. inter-group embedding disparity, and cluster-level demographic parity. The expected outcome is a rigorous understanding of whether fairness properties learned in one hospital setting preserve, degrade, or improve when transferred to another, revealing how robust model fairness is to realworld clinical domain shifts. A practical use case involves a healthcare network seeking to deploy a model trained at a major academic hospital (e.g., Stanford) into a community hospital setting: this project helps determine whether the transferred representations remain equitable across patient groups such as older adults, women, or specific disease cohorts. The findings will support responsible AI deployment in healthcare by highlighting the conditions under which fairness is stable across institutions and identifying scenarios where domain-specific mitigation strategies may be required."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "uhUKfIU5G_5u",
    "ExecuteTime": {
     "end_time": "2025-11-28T09:00:32.475530Z",
     "start_time": "2025-11-28T09:00:30.221768Z"
    }
   },
   "source": [
    "#Process is probably different on colab, this is hyperspecific to me because I'm working on Pycharm connected to my WSL\n",
    "import sys\n",
    "import os\n",
    "!{sys.executable} -m pip install kagglehub polars\n",
    "#We're going to use polars because it's significantly faster, it's build on rust and enables multi-threaded processing as well as some memory optimizations over pandas.\n",
    "\n",
    "# Install PyTorch with CUDA 13.0 support (LINUX Link (May work for windows??))\n",
    "!{sys.executable} -m pip install torch torchvision --index-url https://download.pytorch.org/whl/cu130\n",
    "\n",
    "# Install PyTorch Lightning and Lightning Bolts for SimCLR\n",
    "!{sys.executable} -m pip install pytorch-lightning lightning-bolts"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: warning: setlocale: LC_ALL: cannot change locale (en_US.UTF-8)\r\n",
      "Requirement already satisfied: kagglehub in /home/coding/.virtualenvs/Fall2025ResearchProject/lib/python3.12/site-packages (0.3.13)\r\n",
      "Requirement already satisfied: polars in /home/coding/.virtualenvs/Fall2025ResearchProject/lib/python3.12/site-packages (1.35.2)\r\n",
      "Requirement already satisfied: packaging in /home/coding/.virtualenvs/Fall2025ResearchProject/lib/python3.12/site-packages (from kagglehub) (25.0)\r\n",
      "Requirement already satisfied: pyyaml in /home/coding/.virtualenvs/Fall2025ResearchProject/lib/python3.12/site-packages (from kagglehub) (6.0.3)\r\n",
      "Requirement already satisfied: requests in /home/coding/.virtualenvs/Fall2025ResearchProject/lib/python3.12/site-packages (from kagglehub) (2.32.5)\r\n",
      "Requirement already satisfied: tqdm in /home/coding/.virtualenvs/Fall2025ResearchProject/lib/python3.12/site-packages (from kagglehub) (4.67.1)\r\n",
      "Requirement already satisfied: polars-runtime-32==1.35.2 in /home/coding/.virtualenvs/Fall2025ResearchProject/lib/python3.12/site-packages (from polars) (1.35.2)\r\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/coding/.virtualenvs/Fall2025ResearchProject/lib/python3.12/site-packages (from requests->kagglehub) (3.4.4)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/coding/.virtualenvs/Fall2025ResearchProject/lib/python3.12/site-packages (from requests->kagglehub) (3.11)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/coding/.virtualenvs/Fall2025ResearchProject/lib/python3.12/site-packages (from requests->kagglehub) (2.5.0)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/coding/.virtualenvs/Fall2025ResearchProject/lib/python3.12/site-packages (from requests->kagglehub) (2025.11.12)\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m25.1.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m25.3\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49m/home/coding/.virtualenvs/Fall2025ResearchProject/bin/python -m pip install --upgrade pip\u001B[0m\r\n",
      "/bin/bash: warning: setlocale: LC_ALL: cannot change locale (en_US.UTF-8)\r\n",
      "Looking in indexes: https://download.pytorch.org/whl/cu130\r\n",
      "Requirement already satisfied: torch in /home/coding/.virtualenvs/Fall2025ResearchProject/lib/python3.12/site-packages (2.9.1+cu130)\r\n",
      "Requirement already satisfied: torchvision in /home/coding/.virtualenvs/Fall2025ResearchProject/lib/python3.12/site-packages (0.24.1+cu130)\r\n",
      "Requirement already satisfied: filelock in /home/coding/.virtualenvs/Fall2025ResearchProject/lib/python3.12/site-packages (from torch) (3.19.1)\r\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /home/coding/.virtualenvs/Fall2025ResearchProject/lib/python3.12/site-packages (from torch) (4.15.0)\r\n",
      "Requirement already satisfied: setuptools in /home/coding/.virtualenvs/Fall2025ResearchProject/lib/python3.12/site-packages (from torch) (80.9.0)\r\n",
      "Requirement already satisfied: sympy>=1.13.3 in /home/coding/.virtualenvs/Fall2025ResearchProject/lib/python3.12/site-packages (from torch) (1.14.0)\r\n",
      "Requirement already satisfied: networkx>=2.5.1 in /home/coding/.virtualenvs/Fall2025ResearchProject/lib/python3.12/site-packages (from torch) (3.5)\r\n",
      "Requirement already satisfied: jinja2 in /home/coding/.virtualenvs/Fall2025ResearchProject/lib/python3.12/site-packages (from torch) (3.1.6)\r\n",
      "Requirement already satisfied: fsspec>=0.8.5 in /home/coding/.virtualenvs/Fall2025ResearchProject/lib/python3.12/site-packages (from torch) (2025.9.0)\r\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc==13.0.48 in /home/coding/.virtualenvs/Fall2025ResearchProject/lib/python3.12/site-packages (from torch) (13.0.48)\r\n",
      "Requirement already satisfied: nvidia-cuda-runtime==13.0.48 in /home/coding/.virtualenvs/Fall2025ResearchProject/lib/python3.12/site-packages (from torch) (13.0.48)\r\n",
      "Requirement already satisfied: nvidia-cuda-cupti==13.0.48 in /home/coding/.virtualenvs/Fall2025ResearchProject/lib/python3.12/site-packages (from torch) (13.0.48)\r\n",
      "Requirement already satisfied: nvidia-cudnn-cu13==9.13.0.50 in /home/coding/.virtualenvs/Fall2025ResearchProject/lib/python3.12/site-packages (from torch) (9.13.0.50)\r\n",
      "Requirement already satisfied: nvidia-cublas==13.0.0.19 in /home/coding/.virtualenvs/Fall2025ResearchProject/lib/python3.12/site-packages (from torch) (13.0.0.19)\r\n",
      "Requirement already satisfied: nvidia-cufft==12.0.0.15 in /home/coding/.virtualenvs/Fall2025ResearchProject/lib/python3.12/site-packages (from torch) (12.0.0.15)\r\n",
      "Requirement already satisfied: nvidia-curand==10.4.0.35 in /home/coding/.virtualenvs/Fall2025ResearchProject/lib/python3.12/site-packages (from torch) (10.4.0.35)\r\n",
      "Requirement already satisfied: nvidia-cusolver==12.0.3.29 in /home/coding/.virtualenvs/Fall2025ResearchProject/lib/python3.12/site-packages (from torch) (12.0.3.29)\r\n",
      "Requirement already satisfied: nvidia-cusparse==12.6.2.49 in /home/coding/.virtualenvs/Fall2025ResearchProject/lib/python3.12/site-packages (from torch) (12.6.2.49)\r\n",
      "Requirement already satisfied: nvidia-cusparselt-cu13==0.8.0 in /home/coding/.virtualenvs/Fall2025ResearchProject/lib/python3.12/site-packages (from torch) (0.8.0)\r\n",
      "Requirement already satisfied: nvidia-nccl-cu13==2.27.7 in /home/coding/.virtualenvs/Fall2025ResearchProject/lib/python3.12/site-packages (from torch) (2.27.7)\r\n",
      "Requirement already satisfied: nvidia-nvshmem-cu13==3.3.24 in /home/coding/.virtualenvs/Fall2025ResearchProject/lib/python3.12/site-packages (from torch) (3.3.24)\r\n",
      "Requirement already satisfied: nvidia-nvtx==13.0.39 in /home/coding/.virtualenvs/Fall2025ResearchProject/lib/python3.12/site-packages (from torch) (13.0.39)\r\n",
      "Requirement already satisfied: nvidia-nvjitlink==13.0.39 in /home/coding/.virtualenvs/Fall2025ResearchProject/lib/python3.12/site-packages (from torch) (13.0.39)\r\n",
      "Requirement already satisfied: nvidia-cufile==1.15.0.42 in /home/coding/.virtualenvs/Fall2025ResearchProject/lib/python3.12/site-packages (from torch) (1.15.0.42)\r\n",
      "Requirement already satisfied: triton==3.5.1 in /home/coding/.virtualenvs/Fall2025ResearchProject/lib/python3.12/site-packages (from torch) (3.5.1)\r\n",
      "Requirement already satisfied: numpy in /home/coding/.virtualenvs/Fall2025ResearchProject/lib/python3.12/site-packages (from torchvision) (2.3.3)\r\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /home/coding/.virtualenvs/Fall2025ResearchProject/lib/python3.12/site-packages (from torchvision) (11.3.0)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/coding/.virtualenvs/Fall2025ResearchProject/lib/python3.12/site-packages (from sympy>=1.13.3->torch) (1.3.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/coding/.virtualenvs/Fall2025ResearchProject/lib/python3.12/site-packages (from jinja2->torch) (3.0.3)\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m25.1.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m25.3\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49m/home/coding/.virtualenvs/Fall2025ResearchProject/bin/python -m pip install --upgrade pip\u001B[0m\r\n",
      "/bin/bash: warning: setlocale: LC_ALL: cannot change locale (en_US.UTF-8)\r\n",
      "Requirement already satisfied: pytorch-lightning in /home/coding/.virtualenvs/Fall2025ResearchProject/lib/python3.12/site-packages (1.9.5)\r\n",
      "Requirement already satisfied: lightning-bolts in /home/coding/.virtualenvs/Fall2025ResearchProject/lib/python3.12/site-packages (0.7.0)\r\n",
      "Requirement already satisfied: numpy>=1.17.2 in /home/coding/.virtualenvs/Fall2025ResearchProject/lib/python3.12/site-packages (from pytorch-lightning) (2.3.3)\r\n",
      "Requirement already satisfied: torch>=1.10.0 in /home/coding/.virtualenvs/Fall2025ResearchProject/lib/python3.12/site-packages (from pytorch-lightning) (2.9.1+cu130)\r\n",
      "Requirement already satisfied: tqdm>=4.57.0 in /home/coding/.virtualenvs/Fall2025ResearchProject/lib/python3.12/site-packages (from pytorch-lightning) (4.67.1)\r\n",
      "Requirement already satisfied: PyYAML>=5.4 in /home/coding/.virtualenvs/Fall2025ResearchProject/lib/python3.12/site-packages (from pytorch-lightning) (6.0.3)\r\n",
      "Requirement already satisfied: fsspec>2021.06.0 in /home/coding/.virtualenvs/Fall2025ResearchProject/lib/python3.12/site-packages (from fsspec[http]>2021.06.0->pytorch-lightning) (2025.9.0)\r\n",
      "Requirement already satisfied: torchmetrics>=0.7.0 in /home/coding/.virtualenvs/Fall2025ResearchProject/lib/python3.12/site-packages (from pytorch-lightning) (1.8.2)\r\n",
      "Requirement already satisfied: packaging>=17.1 in /home/coding/.virtualenvs/Fall2025ResearchProject/lib/python3.12/site-packages (from pytorch-lightning) (25.0)\r\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in /home/coding/.virtualenvs/Fall2025ResearchProject/lib/python3.12/site-packages (from pytorch-lightning) (4.15.0)\r\n",
      "Requirement already satisfied: lightning-utilities>=0.6.0.post0 in /home/coding/.virtualenvs/Fall2025ResearchProject/lib/python3.12/site-packages (from pytorch-lightning) (0.15.2)\r\n",
      "Requirement already satisfied: torchvision>=0.10.0 in /home/coding/.virtualenvs/Fall2025ResearchProject/lib/python3.12/site-packages (from lightning-bolts) (0.24.1+cu130)\r\n",
      "Requirement already satisfied: tensorboard>=2.9.1 in /home/coding/.virtualenvs/Fall2025ResearchProject/lib/python3.12/site-packages (from lightning-bolts) (2.20.0)\r\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /home/coding/.virtualenvs/Fall2025ResearchProject/lib/python3.12/site-packages (from fsspec[http]>2021.06.0->pytorch-lightning) (3.13.2)\r\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /home/coding/.virtualenvs/Fall2025ResearchProject/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (2.6.1)\r\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /home/coding/.virtualenvs/Fall2025ResearchProject/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (1.4.0)\r\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/coding/.virtualenvs/Fall2025ResearchProject/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (25.4.0)\r\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/coding/.virtualenvs/Fall2025ResearchProject/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (1.8.0)\r\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/coding/.virtualenvs/Fall2025ResearchProject/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (6.7.0)\r\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/coding/.virtualenvs/Fall2025ResearchProject/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (0.4.1)\r\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/coding/.virtualenvs/Fall2025ResearchProject/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (1.22.0)\r\n",
      "Requirement already satisfied: idna>=2.0 in /home/coding/.virtualenvs/Fall2025ResearchProject/lib/python3.12/site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (3.11)\r\n",
      "Requirement already satisfied: setuptools in /home/coding/.virtualenvs/Fall2025ResearchProject/lib/python3.12/site-packages (from lightning-utilities>=0.6.0.post0->pytorch-lightning) (80.9.0)\r\n",
      "Requirement already satisfied: absl-py>=0.4 in /home/coding/.virtualenvs/Fall2025ResearchProject/lib/python3.12/site-packages (from tensorboard>=2.9.1->lightning-bolts) (2.3.1)\r\n",
      "Requirement already satisfied: grpcio>=1.48.2 in /home/coding/.virtualenvs/Fall2025ResearchProject/lib/python3.12/site-packages (from tensorboard>=2.9.1->lightning-bolts) (1.76.0)\r\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/coding/.virtualenvs/Fall2025ResearchProject/lib/python3.12/site-packages (from tensorboard>=2.9.1->lightning-bolts) (3.10)\r\n",
      "Requirement already satisfied: pillow in /home/coding/.virtualenvs/Fall2025ResearchProject/lib/python3.12/site-packages (from tensorboard>=2.9.1->lightning-bolts) (11.3.0)\r\n",
      "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /home/coding/.virtualenvs/Fall2025ResearchProject/lib/python3.12/site-packages (from tensorboard>=2.9.1->lightning-bolts) (6.33.1)\r\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /home/coding/.virtualenvs/Fall2025ResearchProject/lib/python3.12/site-packages (from tensorboard>=2.9.1->lightning-bolts) (0.7.2)\r\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /home/coding/.virtualenvs/Fall2025ResearchProject/lib/python3.12/site-packages (from tensorboard>=2.9.1->lightning-bolts) (3.1.3)\r\n",
      "Requirement already satisfied: filelock in /home/coding/.virtualenvs/Fall2025ResearchProject/lib/python3.12/site-packages (from torch>=1.10.0->pytorch-lightning) (3.19.1)\r\n",
      "Requirement already satisfied: sympy>=1.13.3 in /home/coding/.virtualenvs/Fall2025ResearchProject/lib/python3.12/site-packages (from torch>=1.10.0->pytorch-lightning) (1.14.0)\r\n",
      "Requirement already satisfied: networkx>=2.5.1 in /home/coding/.virtualenvs/Fall2025ResearchProject/lib/python3.12/site-packages (from torch>=1.10.0->pytorch-lightning) (3.5)\r\n",
      "Requirement already satisfied: jinja2 in /home/coding/.virtualenvs/Fall2025ResearchProject/lib/python3.12/site-packages (from torch>=1.10.0->pytorch-lightning) (3.1.6)\r\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc==13.0.48 in /home/coding/.virtualenvs/Fall2025ResearchProject/lib/python3.12/site-packages (from torch>=1.10.0->pytorch-lightning) (13.0.48)\r\n",
      "Requirement already satisfied: nvidia-cuda-runtime==13.0.48 in /home/coding/.virtualenvs/Fall2025ResearchProject/lib/python3.12/site-packages (from torch>=1.10.0->pytorch-lightning) (13.0.48)\r\n",
      "Requirement already satisfied: nvidia-cuda-cupti==13.0.48 in /home/coding/.virtualenvs/Fall2025ResearchProject/lib/python3.12/site-packages (from torch>=1.10.0->pytorch-lightning) (13.0.48)\r\n",
      "Requirement already satisfied: nvidia-cudnn-cu13==9.13.0.50 in /home/coding/.virtualenvs/Fall2025ResearchProject/lib/python3.12/site-packages (from torch>=1.10.0->pytorch-lightning) (9.13.0.50)\r\n",
      "Requirement already satisfied: nvidia-cublas==13.0.0.19 in /home/coding/.virtualenvs/Fall2025ResearchProject/lib/python3.12/site-packages (from torch>=1.10.0->pytorch-lightning) (13.0.0.19)\r\n",
      "Requirement already satisfied: nvidia-cufft==12.0.0.15 in /home/coding/.virtualenvs/Fall2025ResearchProject/lib/python3.12/site-packages (from torch>=1.10.0->pytorch-lightning) (12.0.0.15)\r\n",
      "Requirement already satisfied: nvidia-curand==10.4.0.35 in /home/coding/.virtualenvs/Fall2025ResearchProject/lib/python3.12/site-packages (from torch>=1.10.0->pytorch-lightning) (10.4.0.35)\r\n",
      "Requirement already satisfied: nvidia-cusolver==12.0.3.29 in /home/coding/.virtualenvs/Fall2025ResearchProject/lib/python3.12/site-packages (from torch>=1.10.0->pytorch-lightning) (12.0.3.29)\r\n",
      "Requirement already satisfied: nvidia-cusparse==12.6.2.49 in /home/coding/.virtualenvs/Fall2025ResearchProject/lib/python3.12/site-packages (from torch>=1.10.0->pytorch-lightning) (12.6.2.49)\r\n",
      "Requirement already satisfied: nvidia-cusparselt-cu13==0.8.0 in /home/coding/.virtualenvs/Fall2025ResearchProject/lib/python3.12/site-packages (from torch>=1.10.0->pytorch-lightning) (0.8.0)\r\n",
      "Requirement already satisfied: nvidia-nccl-cu13==2.27.7 in /home/coding/.virtualenvs/Fall2025ResearchProject/lib/python3.12/site-packages (from torch>=1.10.0->pytorch-lightning) (2.27.7)\r\n",
      "Requirement already satisfied: nvidia-nvshmem-cu13==3.3.24 in /home/coding/.virtualenvs/Fall2025ResearchProject/lib/python3.12/site-packages (from torch>=1.10.0->pytorch-lightning) (3.3.24)\r\n",
      "Requirement already satisfied: nvidia-nvtx==13.0.39 in /home/coding/.virtualenvs/Fall2025ResearchProject/lib/python3.12/site-packages (from torch>=1.10.0->pytorch-lightning) (13.0.39)\r\n",
      "Requirement already satisfied: nvidia-nvjitlink==13.0.39 in /home/coding/.virtualenvs/Fall2025ResearchProject/lib/python3.12/site-packages (from torch>=1.10.0->pytorch-lightning) (13.0.39)\r\n",
      "Requirement already satisfied: nvidia-cufile==1.15.0.42 in /home/coding/.virtualenvs/Fall2025ResearchProject/lib/python3.12/site-packages (from torch>=1.10.0->pytorch-lightning) (1.15.0.42)\r\n",
      "Requirement already satisfied: triton==3.5.1 in /home/coding/.virtualenvs/Fall2025ResearchProject/lib/python3.12/site-packages (from torch>=1.10.0->pytorch-lightning) (3.5.1)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/coding/.virtualenvs/Fall2025ResearchProject/lib/python3.12/site-packages (from sympy>=1.13.3->torch>=1.10.0->pytorch-lightning) (1.3.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /home/coding/.virtualenvs/Fall2025ResearchProject/lib/python3.12/site-packages (from werkzeug>=1.0.1->tensorboard>=2.9.1->lightning-bolts) (3.0.3)\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m25.1.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m25.3\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49m/home/coding/.virtualenvs/Fall2025ResearchProject/bin/python -m pip install --upgrade pip\u001B[0m\r\n"
     ]
    }
   ],
   "execution_count": 141
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-28T09:00:32.482707Z",
     "start_time": "2025-11-28T09:00:32.479801Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Imports\n",
    "from PIL import Image\n",
    "import kagglehub\n",
    "import polars as pol\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "import torchvision.models as models\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, LearningRateMonitor\n",
    "from pytorch_lightning.loggers import TensorBoardLogger"
   ],
   "outputs": [],
   "execution_count": 142
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-28T09:00:32.528797Z",
     "start_time": "2025-11-28T09:00:32.526331Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Hyperparameters\n",
    "base_model=\"resnet50\"\n",
    "max_epochs = 20\n",
    "proj_dim = 128 # How many dimensions to project the image into\n",
    "base_lr = 1e-3\n",
    "temperature = 0.5\n",
    "batch_size = 128\n",
    "\n",
    "num_workers = 12 # This one just effects cpu utilization/training time\n"
   ],
   "outputs": [],
   "execution_count": 143
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-28T09:00:33.097058Z",
     "start_time": "2025-11-28T09:00:32.574847Z"
    }
   },
   "source": [
    "#Again, this is probably different on colab\n",
    "\n",
    "path_chexpert = kagglehub.dataset_download(\"mimsadiislam/chexpert\")\n",
    "print(\"Path to chexpert dataset files:\", path_chexpert)\n",
    "path_mimic = kagglehub.dataset_download(\"simhadrisadaram/mimic-cxr-dataset\")\n",
    "print(\"Path to mimic dataset files:\", path_mimic)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to chexpert dataset files: /home/coding/.cache/kagglehub/datasets/mimsadiislam/chexpert/versions/1\n",
      "Path to mimic dataset files: /home/coding/.cache/kagglehub/datasets/simhadrisadaram/mimic-cxr-dataset/versions/2\n"
     ]
    }
   ],
   "execution_count": 144
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-28T09:00:33.625604Z",
     "start_time": "2025-11-28T09:00:33.100667Z"
    }
   },
   "source": "dir_chexpert = os.path.join(path_chexpert, \"CheXpert-v1.0-small\")\ndir_mimic = os.path.join(path_mimic, \"official_data_iccv_final\")  # Fixed: images are in official_data_iccv_final/files/\n\ntrain_csv_chexpert = os.path.join(dir_chexpert, \"train.csv\")\ntrain_csv_mimic = os.path.join(path_mimic, \"mimic_cxr_aug_train.csv\")  # CSV is at root level\nvalid_csv_chexpert = os.path.join(dir_chexpert, \"valid.csv\")\nvalid_csv_mimic = os.path.join(path_mimic, \"mimic_cxr_aug_validate.csv\")  # CSV is at root level\n\ndf_train_chexpert = pol.read_csv(train_csv_chexpert)\ndf_train_mimic = pol.read_csv(train_csv_mimic)\ndf_valid_chexpert = pol.read_csv(valid_csv_chexpert)\ndf_valid_mimic = pol.read_csv(valid_csv_mimic)",
   "outputs": [],
   "execution_count": 145
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-28T09:00:33.637759Z",
     "start_time": "2025-11-28T09:00:33.632208Z"
    }
   },
   "cell_type": "code",
   "source": "df_train_chexpert.head()",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "shape: (5, 19)\n",
       "┌──────────────────┬────────┬─────┬──────────────────┬───┬──────────┬─────────┬──────────┬─────────┐\n",
       "│ Path             ┆ Sex    ┆ Age ┆ Frontal/Lateral  ┆ … ┆ Pleural  ┆ Pleural ┆ Fracture ┆ Support │\n",
       "│ ---              ┆ ---    ┆ --- ┆ ---              ┆   ┆ Effusion ┆ Other   ┆ ---      ┆ Devices │\n",
       "│ str              ┆ str    ┆ i64 ┆ str              ┆   ┆ ---      ┆ ---     ┆ f64      ┆ ---     │\n",
       "│                  ┆        ┆     ┆                  ┆   ┆ f64      ┆ f64     ┆          ┆ f64     │\n",
       "╞══════════════════╪════════╪═════╪══════════════════╪═══╪══════════╪═════════╪══════════╪═════════╡\n",
       "│ CheXpert-v1.0-sm ┆ Female ┆ 68  ┆ Frontal          ┆ … ┆ null     ┆ null    ┆ null     ┆ 1.0     │\n",
       "│ all/train/pati…  ┆        ┆     ┆                  ┆   ┆          ┆         ┆          ┆         │\n",
       "│ CheXpert-v1.0-sm ┆ Female ┆ 87  ┆ Frontal          ┆ … ┆ -1.0     ┆ null    ┆ 1.0      ┆ null    │\n",
       "│ all/train/pati…  ┆        ┆     ┆                  ┆   ┆          ┆         ┆          ┆         │\n",
       "│ CheXpert-v1.0-sm ┆ Female ┆ 83  ┆ Frontal          ┆ … ┆ null     ┆ null    ┆ 1.0      ┆ null    │\n",
       "│ all/train/pati…  ┆        ┆     ┆                  ┆   ┆          ┆         ┆          ┆         │\n",
       "│ CheXpert-v1.0-sm ┆ Female ┆ 83  ┆ Lateral          ┆ … ┆ null     ┆ null    ┆ 1.0      ┆ null    │\n",
       "│ all/train/pati…  ┆        ┆     ┆                  ┆   ┆          ┆         ┆          ┆         │\n",
       "│ CheXpert-v1.0-sm ┆ Male   ┆ 41  ┆ Frontal          ┆ … ┆ null     ┆ null    ┆ null     ┆ null    │\n",
       "│ all/train/pati…  ┆        ┆     ┆                  ┆   ┆          ┆         ┆          ┆         │\n",
       "└──────────────────┴────────┴─────┴──────────────────┴───┴──────────┴─────────┴──────────┴─────────┘"
      ],
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 19)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>Path</th><th>Sex</th><th>Age</th><th>Frontal/Lateral</th><th>AP/PA</th><th>No Finding</th><th>Enlarged Cardiomediastinum</th><th>Cardiomegaly</th><th>Lung Opacity</th><th>Lung Lesion</th><th>Edema</th><th>Consolidation</th><th>Pneumonia</th><th>Atelectasis</th><th>Pneumothorax</th><th>Pleural Effusion</th><th>Pleural Other</th><th>Fracture</th><th>Support Devices</th></tr><tr><td>str</td><td>str</td><td>i64</td><td>str</td><td>str</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td></tr></thead><tbody><tr><td>&quot;CheXpert-v1.0-small/train/pati…</td><td>&quot;Female&quot;</td><td>68</td><td>&quot;Frontal&quot;</td><td>&quot;AP&quot;</td><td>1.0</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>0.0</td><td>null</td><td>null</td><td>null</td><td>1.0</td></tr><tr><td>&quot;CheXpert-v1.0-small/train/pati…</td><td>&quot;Female&quot;</td><td>87</td><td>&quot;Frontal&quot;</td><td>&quot;AP&quot;</td><td>null</td><td>null</td><td>-1.0</td><td>1.0</td><td>null</td><td>-1.0</td><td>-1.0</td><td>null</td><td>-1.0</td><td>null</td><td>-1.0</td><td>null</td><td>1.0</td><td>null</td></tr><tr><td>&quot;CheXpert-v1.0-small/train/pati…</td><td>&quot;Female&quot;</td><td>83</td><td>&quot;Frontal&quot;</td><td>&quot;AP&quot;</td><td>null</td><td>null</td><td>null</td><td>1.0</td><td>null</td><td>null</td><td>-1.0</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>1.0</td><td>null</td></tr><tr><td>&quot;CheXpert-v1.0-small/train/pati…</td><td>&quot;Female&quot;</td><td>83</td><td>&quot;Lateral&quot;</td><td>null</td><td>null</td><td>null</td><td>null</td><td>1.0</td><td>null</td><td>null</td><td>-1.0</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>1.0</td><td>null</td></tr><tr><td>&quot;CheXpert-v1.0-small/train/pati…</td><td>&quot;Male&quot;</td><td>41</td><td>&quot;Frontal&quot;</td><td>&quot;AP&quot;</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>1.0</td><td>null</td><td>null</td><td>null</td><td>0.0</td><td>null</td><td>null</td><td>null</td><td>null</td></tr></tbody></table></div>"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 146
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-28T09:00:33.752152Z",
     "start_time": "2025-11-28T09:00:33.747932Z"
    }
   },
   "source": [
    "df_train_mimic.head()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "shape: (5, 10)\n",
       "┌───────────┬───────────┬───────────┬───────────┬───┬───────────┬───────────┬───────────┬──────────┐\n",
       "│ Unnamed:  ┆ Unnamed:  ┆ subject_i ┆ image     ┆ … ┆ PA        ┆ Lateral   ┆ text      ┆ text_aug │\n",
       "│ 0.1       ┆ 0         ┆ d         ┆ ---       ┆   ┆ ---       ┆ ---       ┆ ---       ┆ ment     │\n",
       "│ ---       ┆ ---       ┆ ---       ┆ str       ┆   ┆ str       ┆ str       ┆ str       ┆ ---      │\n",
       "│ i64       ┆ i64       ┆ i64       ┆           ┆   ┆           ┆           ┆           ┆ str      │\n",
       "╞═══════════╪═══════════╪═══════════╪═══════════╪═══╪═══════════╪═══════════╪═══════════╪══════════╡\n",
       "│ 0         ┆ 0         ┆ 10000032  ┆ ['files/p ┆ … ┆ ['files/p ┆ ['files/p ┆ ['Finding ┆ ['Findin │\n",
       "│           ┆           ┆           ┆ 10/p10000 ┆   ┆ 10/p10000 ┆ 10/p10000 ┆ s: There  ┆ gs:      │\n",
       "│           ┆           ┆           ┆ 032/s5041 ┆   ┆ 032/s5041 ┆ 032/s5041 ┆ is no     ┆ There is │\n",
       "│           ┆           ┆           ┆ 426…      ┆   ┆ 426…      ┆ 426…      ┆ focal …   ┆ no       │\n",
       "│           ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆ focus,…  │\n",
       "│ 1         ┆ 1         ┆ 10000764  ┆ ['files/p ┆ … ┆ []        ┆ ['files/p ┆ ['Finding ┆ ['Finds: │\n",
       "│           ┆           ┆           ┆ 10/p10000 ┆   ┆           ┆ 10/p10000 ┆ s: PA and ┆ PA and   │\n",
       "│           ┆           ┆           ┆ 764/s5737 ┆   ┆           ┆ 764/s5737 ┆ lateral   ┆ lateral  │\n",
       "│           ┆           ┆           ┆ 596…      ┆   ┆           ┆ 596…      ┆ vie…      ┆ view o…  │\n",
       "│ 2         ┆ 2         ┆ 10000898  ┆ ['files/p ┆ … ┆ ['files/p ┆ ['files/p ┆ ['Finding ┆ ['Finds: │\n",
       "│           ┆           ┆           ┆ 10/p10000 ┆   ┆ 10/p10000 ┆ 10/p10000 ┆ s: PA and ┆ PA and   │\n",
       "│           ┆           ┆           ┆ 898/s5077 ┆   ┆ 898/s5077 ┆ 898/s5077 ┆ lateral   ┆ side     │\n",
       "│           ┆           ┆           ┆ 138…      ┆   ┆ 138…      ┆ 138…      ┆ vie…      ┆ view of  │\n",
       "│           ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆ t…       │\n",
       "│ 3         ┆ 3         ┆ 10000935  ┆ ['files/p ┆ … ┆ ['files/p ┆ ['files/p ┆ ['Finding ┆ ['Result │\n",
       "│           ┆           ┆           ┆ 10/p10000 ┆   ┆ 10/p10000 ┆ 10/p10000 ┆ s: Lung   ┆ s: Pulmo │\n",
       "│           ┆           ┆           ┆ 935/s5057 ┆   ┆ 935/s5569 ┆ 935/s5117 ┆ volumes   ┆ nary     │\n",
       "│           ┆           ┆           ┆ 897…      ┆   ┆ 729…      ┆ 837…      ┆ remai…    ┆ volumes  │\n",
       "│           ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆ r…       │\n",
       "│ 4         ┆ 4         ┆ 10000980  ┆ ['files/p ┆ … ┆ ['files/p ┆ ['files/p ┆ ['Finding ┆ ['Findin │\n",
       "│           ┆           ┆           ┆ 10/p10000 ┆   ┆ 10/p10000 ┆ 10/p10000 ┆ s:  Impre ┆ gs: Impr │\n",
       "│           ┆           ┆           ┆ 980/s5098 ┆   ┆ 980/s5098 ┆ 980/s5457 ┆ ssion:    ┆ ession:  │\n",
       "│           ┆           ┆           ┆ 509…      ┆   ┆ 509…      ┆ 736…      ┆ Compa…    ┆ Compar…  │\n",
       "└───────────┴───────────┴───────────┴───────────┴───┴───────────┴───────────┴───────────┴──────────┘"
      ],
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 10)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>Unnamed: 0.1</th><th>Unnamed: 0</th><th>subject_id</th><th>image</th><th>view</th><th>AP</th><th>PA</th><th>Lateral</th><th>text</th><th>text_augment</th></tr><tr><td>i64</td><td>i64</td><td>i64</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td></tr></thead><tbody><tr><td>0</td><td>0</td><td>10000032</td><td>&quot;[&#x27;files/p10/p10000032/s5041426…</td><td>&quot;[&#x27;PA&#x27;, &#x27;LATERAL&#x27;, &#x27;AP&#x27;]&quot;</td><td>&quot;[&#x27;files/p10/p10000032/s5391176…</td><td>&quot;[&#x27;files/p10/p10000032/s5041426…</td><td>&quot;[&#x27;files/p10/p10000032/s5041426…</td><td>&quot;[&#x27;Findings: There is no focal …</td><td>&quot;[&#x27;Findings: There is no focus,…</td></tr><tr><td>1</td><td>1</td><td>10000764</td><td>&quot;[&#x27;files/p10/p10000764/s5737596…</td><td>&quot;[&#x27;AP&#x27;, &#x27;LATERAL&#x27;]&quot;</td><td>&quot;[&#x27;files/p10/p10000764/s5737596…</td><td>&quot;[]&quot;</td><td>&quot;[&#x27;files/p10/p10000764/s5737596…</td><td>&quot;[&#x27;Findings: PA and lateral vie…</td><td>&quot;[&#x27;Finds: PA and lateral view o…</td></tr><tr><td>2</td><td>2</td><td>10000898</td><td>&quot;[&#x27;files/p10/p10000898/s5077138…</td><td>&quot;[&#x27;LATERAL&#x27;, &#x27;PA&#x27;]&quot;</td><td>&quot;[]&quot;</td><td>&quot;[&#x27;files/p10/p10000898/s5077138…</td><td>&quot;[&#x27;files/p10/p10000898/s5077138…</td><td>&quot;[&#x27;Findings: PA and lateral vie…</td><td>&quot;[&#x27;Finds: PA and side view of t…</td></tr><tr><td>3</td><td>3</td><td>10000935</td><td>&quot;[&#x27;files/p10/p10000935/s5057897…</td><td>&quot;[&#x27;AP&#x27;, &#x27;LATERAL&#x27;, &#x27;LL&#x27;, &#x27;PA&#x27;]&quot;</td><td>&quot;[&#x27;files/p10/p10000935/s5057897…</td><td>&quot;[&#x27;files/p10/p10000935/s5569729…</td><td>&quot;[&#x27;files/p10/p10000935/s5117837…</td><td>&quot;[&#x27;Findings: Lung volumes remai…</td><td>&quot;[&#x27;Results: Pulmonary volumes r…</td></tr><tr><td>4</td><td>4</td><td>10000980</td><td>&quot;[&#x27;files/p10/p10000980/s5098509…</td><td>&quot;[&#x27;PA&#x27;, &#x27;LL&#x27;, &#x27;AP&#x27;, &#x27;LATERAL&#x27;]&quot;</td><td>&quot;[&#x27;files/p10/p10000980/s5196728…</td><td>&quot;[&#x27;files/p10/p10000980/s5098509…</td><td>&quot;[&#x27;files/p10/p10000980/s5457736…</td><td>&quot;[&#x27;Findings:&nbsp;&nbsp;Impression: Compa…</td><td>&quot;[&#x27;Findings: Impression: Compar…</td></tr></tbody></table></div>"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 147
  },
  {
   "cell_type": "code",
   "source": [
    "# Check GPU availability\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"Number of GPUs: {torch.cuda.device_count()}\")\n",
    "    print(f\"Current GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    print(\"No GPU available, using CPU\")\n",
    "    device = torch.device(\"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-28T09:00:33.982034Z",
     "start_time": "2025-11-28T09:00:33.978464Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.9.1+cu130\n",
      "CUDA available: True\n",
      "CUDA version: 13.0\n",
      "Number of GPUs: 1\n",
      "Current GPU: NVIDIA GeForce RTX 5080\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "execution_count": 148
  },
  {
   "cell_type": "code",
   "source": [
    "# Use Tensor Cores efficiently on 5080\n",
    "torch.set_float32_matmul_precision(\"high\")\n",
    "\n",
    "import ast\n",
    "\n",
    "# -----------------------\n",
    "# SimCLR augmentations\n",
    "# -----------------------\n",
    "simclr_transform = T.Compose([\n",
    "    T.Resize(256),\n",
    "    T.RandomResizedCrop(224, scale=(0.2, 1.0)),\n",
    "    T.RandomHorizontalFlip(p=0.5),\n",
    "    T.RandomApply(\n",
    "        [T.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.1)],\n",
    "        p=0.8,\n",
    "    ),\n",
    "    T.RandomGrayscale(p=0.2),\n",
    "    T.ToTensor(),\n",
    "    # CXRs are essentially grayscale; simple normalization is fine as a start\n",
    "    T.Normalize(mean=[0.5], std=[0.5]),\n",
    "])\n",
    "\n",
    "\n",
    "# -----------------------\n",
    "# Single-dataset SimCLR Dataset\n",
    "# -----------------------\n",
    "class SimCLRDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Unlabeled dataset for SimCLR: returns (view1, view2) per image.\n",
    "    This handles ONE dataset (CheXpert OR MIMIC).\n",
    "    \"\"\"\n",
    "    def __init__(self, df, root, path_col, transform=None, is_mimic=False):\n",
    "        self.transform = transform\n",
    "        self.is_mimic = is_mimic\n",
    "        \n",
    "        # df is a Polars DataFrame; df[path_col] is a Series → .to_list()\n",
    "        rel_paths = df[path_col].to_list()\n",
    "        \n",
    "        # Parse MIMIC paths if needed (they're stored as string lists)\n",
    "        if is_mimic:\n",
    "            parsed_paths = []\n",
    "            for p in rel_paths:\n",
    "                # Parse string representation of list: \"['files/p16/...']\" -> 'files/p16/...'\n",
    "                if isinstance(p, str) and p.startswith('['):\n",
    "                    try:\n",
    "                        path_list = ast.literal_eval(p)\n",
    "                        if path_list and len(path_list) > 0:\n",
    "                            parsed_paths.append(path_list[0])  # Take first element\n",
    "                    except:\n",
    "                        print(f\"Warning: Could not parse path: {p}\")\n",
    "                else:\n",
    "                    parsed_paths.append(p)\n",
    "            rel_paths = parsed_paths\n",
    "        \n",
    "        self.paths = [os.path.join(root, p) for p in rel_paths]\n",
    "\n",
    "        print(f\"Total images for this SimCLR dataset: {len(self.paths)}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.paths[idx]\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "        if self.transform is None:\n",
    "            raise ValueError(\"SimCLRDataset requires a transform.\")\n",
    "\n",
    "        xi = self.transform(img)\n",
    "        xj = self.transform(img)\n",
    "        return xi, xj\n",
    "\n",
    "\n",
    "# -----------------------\n",
    "# Build DataLoaders\n",
    "# -----------------------\n",
    "\n",
    "simclr_train_chexpert = SimCLRDataset(\n",
    "    df=df_train_chexpert,\n",
    "    root=path_chexpert,\n",
    "    path_col=\"Path\",      # CheXpert image path column\n",
    "    transform=simclr_transform,\n",
    "    is_mimic=False,\n",
    ")\n",
    "\n",
    "simclr_train_loader_chexpert = DataLoader(\n",
    "    simclr_train_chexpert,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers,\n",
    "    drop_last=True,\n",
    ")\n",
    "\n",
    "simclr_train_mimic = SimCLRDataset(\n",
    "    df=df_train_mimic,\n",
    "    root=dir_mimic,\n",
    "    path_col=\"image\",     # MIMIC image path column\n",
    "    transform=simclr_transform,\n",
    "    is_mimic=True,        # Enable MIMIC path parsing\n",
    ")\n",
    "\n",
    "simclr_train_loader_mimic = DataLoader(\n",
    "    simclr_train_mimic,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers,\n",
    "    drop_last=True,\n",
    ")"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-28T09:00:34.916334Z",
     "start_time": "2025-11-28T09:00:34.037687Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total images for this SimCLR dataset: 223414\n",
      "Total images for this SimCLR dataset: 64586\n"
     ]
    }
   ],
   "execution_count": 149
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-28T09:00:34.971684Z",
     "start_time": "2025-11-28T09:00:34.965572Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class SimCLR(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        base_model: str = \"resnet34\",\n",
    "        out_dim: int = 128,\n",
    "        lr: float = 1e-3,\n",
    "        temperature: float = 0.5,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        # -------- Encoder (backbone) --------\n",
    "        if base_model == \"resnet34\":\n",
    "            backbone = models.resnet34(weights=models.ResNet34_Weights.IMAGENET1K_V1)\n",
    "        elif base_model == \"resnet50\":\n",
    "            backbone = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V2)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported base_model: {base_model}\")\n",
    "\n",
    "        feat_dim = backbone.fc.in_features\n",
    "        backbone.fc = nn.Identity()\n",
    "        self.encoder = backbone\n",
    "\n",
    "        # -------- Projection head --------\n",
    "        self.projector = nn.Sequential(\n",
    "            nn.Linear(feat_dim, feat_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(feat_dim, out_dim),\n",
    "        )\n",
    "\n",
    "        self.temperature = temperature\n",
    "        self.lr = lr\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.encoder(x)           # [B, feat_dim]\n",
    "        z = self.projector(h)         # [B, out_dim]\n",
    "        z = F.normalize(z, dim=1)     # L2-normalize\n",
    "        return z\n",
    "\n",
    "    def nt_xent_loss(self, z_i, z_j):\n",
    "        \"\"\"\n",
    "        NT-Xent (InfoNCE) loss used in SimCLR.\n",
    "        z_i, z_j: [B, D] normalized embeddings for the two views\n",
    "        \"\"\"\n",
    "        batch_size = z_i.size(0)\n",
    "        z = torch.cat([z_i, z_j], dim=0)   # [2B, D]\n",
    "\n",
    "        # Cosine similarity matrix: [2B, 2B]\n",
    "        sim = F.cosine_similarity(z.unsqueeze(1), z.unsqueeze(0), dim=2)\n",
    "        sim = sim / self.temperature\n",
    "\n",
    "        # Mask out self-similarity on diagonal\n",
    "        self_mask = torch.eye(2 * batch_size, dtype=torch.bool, device=z.device)\n",
    "        sim = sim.masked_fill(self_mask, float(\"-inf\"))\n",
    "\n",
    "        # Positive for each i is i+batch_size (wrap around)\n",
    "        pos_indices = torch.arange(2 * batch_size, device=z.device)\n",
    "        pos_indices = (pos_indices + batch_size) % (2 * batch_size)\n",
    "\n",
    "        loss = F.cross_entropy(sim, pos_indices)\n",
    "        return loss\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x_i, x_j = batch\n",
    "        z_i = self(x_i)\n",
    "        z_j = self(x_j)\n",
    "        loss = self.nt_xent_loss(z_i, z_j)\n",
    "        self.log(\"train_loss\", loss, prog_bar=True, on_step=True, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(self.parameters(), lr=self.lr)\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "            optimizer, T_max=self.trainer.max_epochs\n",
    "        )\n",
    "        return {\"optimizer\": optimizer, \"lr_scheduler\": scheduler}\n"
   ],
   "outputs": [],
   "execution_count": 150
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-28T09:00:35.892933Z",
     "start_time": "2025-11-28T09:00:35.015733Z"
    }
   },
   "cell_type": "code",
   "source": [
    "lr_monitor = LearningRateMonitor(logging_interval=\"epoch\")\n",
    "\n",
    "# -----------------------\n",
    "# Loggers\n",
    "# -----------------------\n",
    "\n",
    "tb_logger_chexpert = TensorBoardLogger(\n",
    "    save_dir=\"tb_logs\",\n",
    "    name=\"simclr_chexpert\"\n",
    ")\n",
    "\n",
    "tb_logger_mimic = TensorBoardLogger(\n",
    "    save_dir=\"tb_logs\",\n",
    "    name=\"simclr_mimic\"\n",
    ")\n",
    "\n",
    "# -----------------------\n",
    "# CheXpert SimCLR\n",
    "# -----------------------\n",
    "simclr_chexpert = SimCLR(\n",
    "    base_model=base_model,\n",
    "    out_dim=proj_dim,\n",
    "    lr=base_lr,\n",
    "    temperature=temperature,\n",
    ")\n",
    "\n",
    "ckpt_chexpert = ModelCheckpoint(\n",
    "    dirpath=\"checkpoints/simclr_chexpert\",\n",
    "    filename=\"simclr-chexpert-{epoch:02d}-{train_loss:.4f}\",\n",
    "    save_top_k=3,\n",
    "    monitor=\"train_loss\",\n",
    "    mode=\"min\",\n",
    "    save_last=True,\n",
    ")\n",
    "\n",
    "trainer_chexpert = pl.Trainer(\n",
    "    max_epochs=max_epochs,\n",
    "    accelerator=\"gpu\" if torch.cuda.is_available() else \"auto\",\n",
    "    devices=1,\n",
    "    precision=\"bf16\" if torch.cuda.is_available() else \"32\",\n",
    "    callbacks=[ckpt_chexpert, lr_monitor],\n",
    "    logger=tb_logger_chexpert,\n",
    "    log_every_n_steps=50,\n",
    ")\n",
    "\n",
    "\n",
    "# -----------------------\n",
    "# MIMIC SimCLR\n",
    "# -----------------------\n",
    "simclr_mimic = SimCLR(\n",
    "    base_model=base_model,\n",
    "    out_dim=proj_dim,\n",
    "    lr=base_lr,\n",
    "    temperature=temperature,\n",
    ")\n",
    "\n",
    "ckpt_mimic = ModelCheckpoint(\n",
    "    dirpath=\"checkpoints/simclr_mimic\",\n",
    "    filename=\"simclr-mimic-{epoch:02d}-{train_loss:.4f}\",\n",
    "    save_top_k=3,\n",
    "    monitor=\"train_loss\",\n",
    "    mode=\"min\",\n",
    "    save_last=True,\n",
    ")\n",
    "\n",
    "trainer_mimic = pl.Trainer(\n",
    "    max_epochs=max_epochs,\n",
    "    accelerator=\"gpu\" if torch.cuda.is_available() else \"auto\",\n",
    "    devices=1,\n",
    "    precision=\"bf16\" if torch.cuda.is_available() else \"32\",\n",
    "    callbacks=[ckpt_mimic, lr_monitor],\n",
    "    logger=tb_logger_mimic,\n",
    "    log_every_n_steps=50,\n",
    ")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using bfloat16 Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Using bfloat16 Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "execution_count": 151
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-28T08:52:03.366951Z",
     "start_time": "2025-11-28T08:52:01.255566Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"=== Training SimCLR on CheXpert ===\")\n",
    "trainer_chexpert.fit(simclr_chexpert, simclr_train_loader_chexpert)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type       | Params\n",
      "-----------------------------------------\n",
      "0 | encoder   | ResNet     | 23.5 M\n",
      "1 | projector | Sequential | 4.5 M \n",
      "-----------------------------------------\n",
      "28.0 M    Trainable params\n",
      "0         Non-trainable params\n",
      "28.0 M    Total params\n",
      "111.867   Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Training SimCLR on CheXpert ===\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "091d80350b2b4a2994718f4610a1dc02"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    }
   ],
   "execution_count": 104
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-28T09:00:38.691082Z",
     "start_time": "2025-11-28T09:00:37.771497Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#There's currently an issue with MIMIC that we need to talk to Blessing about - The CSV references the full set but the downloaded images are a subset. Two options: 1. We just work off the subset. 2. We talk to him about getting the full set\n",
    "print(\"=== Training SimCLR on MIMIC ===\")\n",
    "trainer_mimic.fit(simclr_mimic, simclr_train_loader_mimic)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type       | Params\n",
      "-----------------------------------------\n",
      "0 | encoder   | ResNet     | 23.5 M\n",
      "1 | projector | Sequential | 4.5 M \n",
      "-----------------------------------------\n",
      "28.0 M    Trainable params\n",
      "0         Non-trainable params\n",
      "28.0 M    Total params\n",
      "111.867   Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Training SimCLR on MIMIC ===\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "320015f13f1a45878faf61f8a8a318a3"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "Caught FileNotFoundError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/coding/.virtualenvs/Fall2025ResearchProject/lib/python3.12/site-packages/torch/utils/data/_utils/worker.py\", line 349, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/home/coding/.virtualenvs/Fall2025ResearchProject/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py\", line 52, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n            ~~~~~~~~~~~~^^^^^\n  File \"/tmp/ipykernel_76730/2523942101.py\", line 64, in __getitem__\n    img = Image.open(img_path).convert(\"RGB\")\n          ^^^^^^^^^^^^^^^^^^^^\n  File \"/home/coding/.virtualenvs/Fall2025ResearchProject/lib/python3.12/site-packages/PIL/Image.py\", line 3513, in open\n    fp = builtins.open(filename, \"rb\")\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nFileNotFoundError: [Errno 2] No such file or directory: '/home/coding/.cache/kagglehub/datasets/simhadrisadaram/mimic-cxr-dataset/versions/2/official_data_iccv_final/files/p15/p15005430/s51648157/5612acc0-ab3c0b31-023f3c8d-6f130c9e-e8372f40.jpg'\n",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mFileNotFoundError\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[152]\u001B[39m\u001B[32m, line 2\u001B[39m\n\u001B[32m      1\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33m\"\u001B[39m\u001B[33m=== Training SimCLR on MIMIC ===\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m----> \u001B[39m\u001B[32m2\u001B[39m \u001B[43mtrainer_mimic\u001B[49m\u001B[43m.\u001B[49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43msimclr_mimic\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msimclr_train_loader_mimic\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.virtualenvs/Fall2025ResearchProject/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py:608\u001B[39m, in \u001B[36mTrainer.fit\u001B[39m\u001B[34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001B[39m\n\u001B[32m    606\u001B[39m model = \u001B[38;5;28mself\u001B[39m._maybe_unwrap_optimized(model)\n\u001B[32m    607\u001B[39m \u001B[38;5;28mself\u001B[39m.strategy._lightning_module = model\n\u001B[32m--> \u001B[39m\u001B[32m608\u001B[39m \u001B[43mcall\u001B[49m\u001B[43m.\u001B[49m\u001B[43m_call_and_handle_interrupt\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    609\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_fit_impl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_dataloaders\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mval_dataloaders\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdatamodule\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mckpt_path\u001B[49m\n\u001B[32m    610\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.virtualenvs/Fall2025ResearchProject/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py:38\u001B[39m, in \u001B[36m_call_and_handle_interrupt\u001B[39m\u001B[34m(trainer, trainer_fn, *args, **kwargs)\u001B[39m\n\u001B[32m     36\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)\n\u001B[32m     37\u001B[39m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m---> \u001B[39m\u001B[32m38\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtrainer_fn\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     40\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m _TunerExitException:\n\u001B[32m     41\u001B[39m     trainer._call_teardown_hook()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.virtualenvs/Fall2025ResearchProject/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py:650\u001B[39m, in \u001B[36mTrainer._fit_impl\u001B[39m\u001B[34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001B[39m\n\u001B[32m    643\u001B[39m ckpt_path = ckpt_path \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m.resume_from_checkpoint\n\u001B[32m    644\u001B[39m \u001B[38;5;28mself\u001B[39m._ckpt_path = \u001B[38;5;28mself\u001B[39m._checkpoint_connector._set_ckpt_path(\n\u001B[32m    645\u001B[39m     \u001B[38;5;28mself\u001B[39m.state.fn,\n\u001B[32m    646\u001B[39m     ckpt_path,  \u001B[38;5;66;03m# type: ignore[arg-type]\u001B[39;00m\n\u001B[32m    647\u001B[39m     model_provided=\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[32m    648\u001B[39m     model_connected=\u001B[38;5;28mself\u001B[39m.lightning_module \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[32m    649\u001B[39m )\n\u001B[32m--> \u001B[39m\u001B[32m650\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_run\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mckpt_path\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mckpt_path\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    652\u001B[39m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m.state.stopped\n\u001B[32m    653\u001B[39m \u001B[38;5;28mself\u001B[39m.training = \u001B[38;5;28;01mFalse\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.virtualenvs/Fall2025ResearchProject/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py:1112\u001B[39m, in \u001B[36mTrainer._run\u001B[39m\u001B[34m(self, model, ckpt_path)\u001B[39m\n\u001B[32m   1108\u001B[39m \u001B[38;5;28mself\u001B[39m._checkpoint_connector.restore_training_state()\n\u001B[32m   1110\u001B[39m \u001B[38;5;28mself\u001B[39m._checkpoint_connector.resume_end()\n\u001B[32m-> \u001B[39m\u001B[32m1112\u001B[39m results = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_run_stage\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1114\u001B[39m log.detail(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m.\u001B[34m__class__\u001B[39m.\u001B[34m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m: trainer tearing down\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m   1115\u001B[39m \u001B[38;5;28mself\u001B[39m._teardown()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.virtualenvs/Fall2025ResearchProject/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py:1191\u001B[39m, in \u001B[36mTrainer._run_stage\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m   1189\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.predicting:\n\u001B[32m   1190\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._run_predict()\n\u001B[32m-> \u001B[39m\u001B[32m1191\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_run_train\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.virtualenvs/Fall2025ResearchProject/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py:1214\u001B[39m, in \u001B[36mTrainer._run_train\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m   1211\u001B[39m \u001B[38;5;28mself\u001B[39m.fit_loop.trainer = \u001B[38;5;28mself\u001B[39m\n\u001B[32m   1213\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m torch.autograd.set_detect_anomaly(\u001B[38;5;28mself\u001B[39m._detect_anomaly):\n\u001B[32m-> \u001B[39m\u001B[32m1214\u001B[39m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mfit_loop\u001B[49m\u001B[43m.\u001B[49m\u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.virtualenvs/Fall2025ResearchProject/lib/python3.12/site-packages/pytorch_lightning/loops/loop.py:199\u001B[39m, in \u001B[36mLoop.run\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m    197\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m    198\u001B[39m     \u001B[38;5;28mself\u001B[39m.on_advance_start(*args, **kwargs)\n\u001B[32m--> \u001B[39m\u001B[32m199\u001B[39m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43madvance\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    200\u001B[39m     \u001B[38;5;28mself\u001B[39m.on_advance_end()\n\u001B[32m    201\u001B[39m     \u001B[38;5;28mself\u001B[39m._restarting = \u001B[38;5;28;01mFalse\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.virtualenvs/Fall2025ResearchProject/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:267\u001B[39m, in \u001B[36mFitLoop.advance\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    265\u001B[39m \u001B[38;5;28mself\u001B[39m._data_fetcher.setup(dataloader, batch_to_device=batch_to_device)\n\u001B[32m    266\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m.trainer.profiler.profile(\u001B[33m\"\u001B[39m\u001B[33mrun_training_epoch\u001B[39m\u001B[33m\"\u001B[39m):\n\u001B[32m--> \u001B[39m\u001B[32m267\u001B[39m     \u001B[38;5;28mself\u001B[39m._outputs = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mepoch_loop\u001B[49m\u001B[43m.\u001B[49m\u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_data_fetcher\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.virtualenvs/Fall2025ResearchProject/lib/python3.12/site-packages/pytorch_lightning/loops/loop.py:199\u001B[39m, in \u001B[36mLoop.run\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m    197\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m    198\u001B[39m     \u001B[38;5;28mself\u001B[39m.on_advance_start(*args, **kwargs)\n\u001B[32m--> \u001B[39m\u001B[32m199\u001B[39m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43madvance\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    200\u001B[39m     \u001B[38;5;28mself\u001B[39m.on_advance_end()\n\u001B[32m    201\u001B[39m     \u001B[38;5;28mself\u001B[39m._restarting = \u001B[38;5;28;01mFalse\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.virtualenvs/Fall2025ResearchProject/lib/python3.12/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py:187\u001B[39m, in \u001B[36mTrainingEpochLoop.advance\u001B[39m\u001B[34m(self, data_fetcher)\u001B[39m\n\u001B[32m    185\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(data_fetcher, DataLoaderIterDataFetcher):\n\u001B[32m    186\u001B[39m     batch_idx = \u001B[38;5;28mself\u001B[39m.batch_idx + \u001B[32m1\u001B[39m\n\u001B[32m--> \u001B[39m\u001B[32m187\u001B[39m     batch = \u001B[38;5;28;43mnext\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mdata_fetcher\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    188\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    189\u001B[39m     batch_idx, batch = \u001B[38;5;28mnext\u001B[39m(data_fetcher)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.virtualenvs/Fall2025ResearchProject/lib/python3.12/site-packages/pytorch_lightning/utilities/fetching.py:184\u001B[39m, in \u001B[36mAbstractDataFetcher.__next__\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    183\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m__next__\u001B[39m(\u001B[38;5;28mself\u001B[39m) -> Any:\n\u001B[32m--> \u001B[39m\u001B[32m184\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mfetching_function\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.virtualenvs/Fall2025ResearchProject/lib/python3.12/site-packages/pytorch_lightning/utilities/fetching.py:265\u001B[39m, in \u001B[36mDataFetcher.fetching_function\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    262\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m.done:\n\u001B[32m    263\u001B[39m     \u001B[38;5;66;03m# this will run only when no pre-fetching was done.\u001B[39;00m\n\u001B[32m    264\u001B[39m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m265\u001B[39m         \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_fetch_next_batch\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mdataloader_iter\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    266\u001B[39m         \u001B[38;5;66;03m# consume the batch we just fetched\u001B[39;00m\n\u001B[32m    267\u001B[39m         batch = \u001B[38;5;28mself\u001B[39m.batches.pop(\u001B[32m0\u001B[39m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.virtualenvs/Fall2025ResearchProject/lib/python3.12/site-packages/pytorch_lightning/utilities/fetching.py:280\u001B[39m, in \u001B[36mDataFetcher._fetch_next_batch\u001B[39m\u001B[34m(self, iterator)\u001B[39m\n\u001B[32m    278\u001B[39m start_output = \u001B[38;5;28mself\u001B[39m.on_fetch_start()\n\u001B[32m    279\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m280\u001B[39m     batch = \u001B[38;5;28;43mnext\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43miterator\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    281\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mStopIteration\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[32m    282\u001B[39m     \u001B[38;5;28mself\u001B[39m._stop_profiler()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.virtualenvs/Fall2025ResearchProject/lib/python3.12/site-packages/pytorch_lightning/trainer/supporters.py:571\u001B[39m, in \u001B[36mCombinedLoaderIterator.__next__\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    565\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m__next__\u001B[39m(\u001B[38;5;28mself\u001B[39m) -> Any:\n\u001B[32m    566\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"Fetches the next batch from multiple data loaders.\u001B[39;00m\n\u001B[32m    567\u001B[39m \n\u001B[32m    568\u001B[39m \u001B[33;03m    Returns:\u001B[39;00m\n\u001B[32m    569\u001B[39m \u001B[33;03m        a collections of batch data\u001B[39;00m\n\u001B[32m    570\u001B[39m \u001B[33;03m    \"\"\"\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m571\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mrequest_next_batch\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mloader_iters\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.virtualenvs/Fall2025ResearchProject/lib/python3.12/site-packages/pytorch_lightning/trainer/supporters.py:583\u001B[39m, in \u001B[36mCombinedLoaderIterator.request_next_batch\u001B[39m\u001B[34m(loader_iters)\u001B[39m\n\u001B[32m    573\u001B[39m \u001B[38;5;129m@staticmethod\u001B[39m\n\u001B[32m    574\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mrequest_next_batch\u001B[39m(loader_iters: Union[Iterator, Sequence, Mapping]) -> Any:\n\u001B[32m    575\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"Return the batch of data from multiple iterators.\u001B[39;00m\n\u001B[32m    576\u001B[39m \n\u001B[32m    577\u001B[39m \u001B[33;03m    Args:\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m    581\u001B[39m \u001B[33;03m        Any: a collections of batch data\u001B[39;00m\n\u001B[32m    582\u001B[39m \u001B[33;03m    \"\"\"\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m583\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mapply_to_collection\u001B[49m\u001B[43m(\u001B[49m\u001B[43mloader_iters\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mIterator\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mnext\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.virtualenvs/Fall2025ResearchProject/lib/python3.12/site-packages/lightning_utilities/core/apply_func.py:66\u001B[39m, in \u001B[36mapply_to_collection\u001B[39m\u001B[34m(data, dtype, function, wrong_dtype, include_none, allow_frozen, *args, **kwargs)\u001B[39m\n\u001B[32m     64\u001B[39m \u001B[38;5;66;03m# fast path for the most common cases:\u001B[39;00m\n\u001B[32m     65\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(data, dtype):  \u001B[38;5;66;03m# single element\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m66\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunction\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     67\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m data.\u001B[34m__class__\u001B[39m \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28mlist\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mall\u001B[39m(\u001B[38;5;28misinstance\u001B[39m(x, dtype) \u001B[38;5;28;01mfor\u001B[39;00m x \u001B[38;5;129;01min\u001B[39;00m data):  \u001B[38;5;66;03m# 1d homogeneous list\u001B[39;00m\n\u001B[32m     68\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m [function(x, *args, **kwargs) \u001B[38;5;28;01mfor\u001B[39;00m x \u001B[38;5;129;01min\u001B[39;00m data]\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.virtualenvs/Fall2025ResearchProject/lib/python3.12/site-packages/torch/utils/data/dataloader.py:732\u001B[39m, in \u001B[36m_BaseDataLoaderIter.__next__\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    729\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m._sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m    730\u001B[39m     \u001B[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[39;00m\n\u001B[32m    731\u001B[39m     \u001B[38;5;28mself\u001B[39m._reset()  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m732\u001B[39m data = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_next_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    733\u001B[39m \u001B[38;5;28mself\u001B[39m._num_yielded += \u001B[32m1\u001B[39m\n\u001B[32m    734\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[32m    735\u001B[39m     \u001B[38;5;28mself\u001B[39m._dataset_kind == _DatasetKind.Iterable\n\u001B[32m    736\u001B[39m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m._IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m    737\u001B[39m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m._num_yielded > \u001B[38;5;28mself\u001B[39m._IterableDataset_len_called\n\u001B[32m    738\u001B[39m ):\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.virtualenvs/Fall2025ResearchProject/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1506\u001B[39m, in \u001B[36m_MultiProcessingDataLoaderIter._next_data\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m   1504\u001B[39m worker_id = \u001B[38;5;28mself\u001B[39m._task_info.pop(idx)[\u001B[32m0\u001B[39m]\n\u001B[32m   1505\u001B[39m \u001B[38;5;28mself\u001B[39m._rcvd_idx += \u001B[32m1\u001B[39m\n\u001B[32m-> \u001B[39m\u001B[32m1506\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_process_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mworker_id\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.virtualenvs/Fall2025ResearchProject/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1541\u001B[39m, in \u001B[36m_MultiProcessingDataLoaderIter._process_data\u001B[39m\u001B[34m(self, data, worker_idx)\u001B[39m\n\u001B[32m   1539\u001B[39m \u001B[38;5;28mself\u001B[39m._try_put_index()\n\u001B[32m   1540\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(data, ExceptionWrapper):\n\u001B[32m-> \u001B[39m\u001B[32m1541\u001B[39m     \u001B[43mdata\u001B[49m\u001B[43m.\u001B[49m\u001B[43mreraise\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1542\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m data\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.virtualenvs/Fall2025ResearchProject/lib/python3.12/site-packages/torch/_utils.py:769\u001B[39m, in \u001B[36mExceptionWrapper.reraise\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    765\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m:\n\u001B[32m    766\u001B[39m     \u001B[38;5;66;03m# If the exception takes multiple arguments or otherwise can't\u001B[39;00m\n\u001B[32m    767\u001B[39m     \u001B[38;5;66;03m# be constructed, don't try to instantiate since we don't know how to\u001B[39;00m\n\u001B[32m    768\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(msg) \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m769\u001B[39m \u001B[38;5;28;01mraise\u001B[39;00m exception\n",
      "\u001B[31mFileNotFoundError\u001B[39m: Caught FileNotFoundError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/coding/.virtualenvs/Fall2025ResearchProject/lib/python3.12/site-packages/torch/utils/data/_utils/worker.py\", line 349, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/home/coding/.virtualenvs/Fall2025ResearchProject/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py\", line 52, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n            ~~~~~~~~~~~~^^^^^\n  File \"/tmp/ipykernel_76730/2523942101.py\", line 64, in __getitem__\n    img = Image.open(img_path).convert(\"RGB\")\n          ^^^^^^^^^^^^^^^^^^^^\n  File \"/home/coding/.virtualenvs/Fall2025ResearchProject/lib/python3.12/site-packages/PIL/Image.py\", line 3513, in open\n    fp = builtins.open(filename, \"rb\")\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nFileNotFoundError: [Errno 2] No such file or directory: '/home/coding/.cache/kagglehub/datasets/simhadrisadaram/mimic-cxr-dataset/versions/2/official_data_iccv_final/files/p15/p15005430/s51648157/5612acc0-ab3c0b31-023f3c8d-6f130c9e-e8372f40.jpg'\n"
     ]
    }
   ],
   "execution_count": 152
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPk3VYQAvkG8XZbveKPAqcC",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  },
  "polyglot_notebook": {
   "kernelInfo": {
    "defaultKernelName": "csharp",
    "items": [
     {
      "aliases": [],
      "name": "csharp"
     }
    ]
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
